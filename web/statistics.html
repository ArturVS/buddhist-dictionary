<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="NTI Buddhist Text Reader">
    <title>NTI Buddhist Text Reader</title>
    <link rel="shortcut icon" href="images/yan.png" type="image/jpeg" />
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="buddhistdict.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="starter-template">
      <div class="row">
        <div class="span2"><img id="logo" src="images/yan.png" alt="Logo" class="pull-left"/></div>
        <div class="span7"><h1>NTI Buddhist Text Reader</h1></div>
      </div>
    </div>
    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Home</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="corpus.html">Texts</a></li>
            <li class="active"><a href="tools.html">Tools</a></li>
            <li><a href="dict_resources.html">Resources</a></li>
            <li><a href="about.html">About</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
      <h2>Statistical Methods</h2>
      <h3>Contents</h3>
      <p>
        <ul>
          <li><a href="#probability">Probability</a></li>
          <li><a href="#data">Corpus Data</a></li>
          <li><a href="#tools">Software Tools</a></li>
          <li><a href="#expectation">Expectation</a></li>
          <li><a href="#special">Special Distributions</a></li>
          <li><a href="#estimation">Statistical Estimation</a></li>
          <li><a href="#references">References</a></li>
        </ul>
      </p>
      <p>
        This page gives some incomplete notes on statistical methods in analyzing Chinese text 
        to solve text analysis, historical linguistcs, and content analysis problems.
      </p>
      <h3><a name="probability"></a>Probability</h2>
      <p>
        The <strong>conditional probability</strong> of an event A given that we know an event B has already occured is written Pr(A|B).
        It can be computed as 
      </p>
      <p class='formula'>
        Pr(A|B) = Pr(A ∩ B) / Pr(B)
      </p>
      <p>
        This assumes that Pr(B) &gt; 0. (DeGroot and Morris, <i>Probability and Statistics</i>, 56)
      </p>
      <p>
        <strong>Example</strong>: Suppose that we wish to find the probability finding the character 說 at a particular position in the text. 
        In modern Chinese, the sequence 我說 is very common.
        So, if we know the previous character is 我 then the probability may be higher than the probability of finding the character 說
        alone. That is, Pr(A=說|B=我) &gt; Pr(A=說).
      </p>
      <p>
        Two events A and B are <strong>independent</strong> if the occurence of one does not affect the occurrence of the other. 
        If this is true then
      </p>
      <p class='formula'>
        Pr(A ∩ B) = Pr(A) Pr(B)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 66)
      </p>
      <p>
        A <strong>random variable</strong> is a real-valued function in a sample space S. (DeGroot and Morris, <i>Probability and Statistics</i>, 93)
      </p>
      <p>
        The <strong>probability function</strong> of a discrete random variable X is the function
      </p>
      <p class='formula'>
        f(x) = Pr(X = x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 96)
        In linguistics discrete random variables, like word frequency, are more common but continuous random
        variables may also be used, for example the word frequency of a specific word per 1,000 words of text.
        The equivalent function for a continuous random variable is called the 
        <strong>probability density function</strong> (pdf).
      </p>
      <p>
        The <strong>cummulative distribution function</strong> (cdf) of a random variable X is 
      </p>
      <p class='formula'>
        F(x) = Pr(X ≤ x) for -∞ &lt; x &lt; ∞
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 108)
      </p>
      <p>
        The <strong>quantile function</strong> F<sup>-1</sup>(p) of a random variable X is the inverse of the cdf,
        which is also the smallest value x  with F(x) ≥ p. The variable p is the probability.
        F<sup>-1</sup>(p) is the p quantile of X or 100p percentile.
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 112)
      </p>
      <p>
        The <strong>joint probability function</strong> of two random variables X and Y is 
      </p>
      <p class='formula'>
        f(x, y) = Pr(X = x and Y = y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 119)
      </p>
      <p>
        The <strong>marginal cdf</strong> of a joint probability function of two discrete random variables X and Y is 
        summed over all possible values of y. In symbols,
      </p>
      <p class='formula'>
        f<sub>1</sub>(x) = Σ<sub>All y</sub> f(x, y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 131)
      </p>
      <p>
        A <strong>stochastic process</strong> is a sequence of random variables X<sub>1</sub>, X<sub>2</sub>, ... at discrete points
        in time. A <strong>Markov chain</strong> is a stochastic process where the conditional distributions of all X<sub>n+j</sub>
        depend only on X<sub>n</sub> and not only earlier states.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 188)
      </p>
      <p>
        The <strong>transition distributions</strong> of a Markov chain are the conditional probabilities 
      </p>
      <p class='formula'>
        p<sub>ij</sub> = Pr(X<sub>n+1</sub>=j|X<sub>n</sub>=i)
      </p>
      <p>
        where the random variables X<sub>n</sub> can have k possible states.
        A <strong>transition matrix</strong> is a matrix <strong>P</strong> = [p<sub>ij</sub>] made up of the conditional probabilities of the 
        transition distributions.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 190-192)
      </p>
      <p>
        <strong>Example</strong>: A stream of words can be thought of as a Markov chain. The earlier words can influence the later
        words. In a simple statistical model, each word may only be influenced by the preceding words. 
      </p>
      <h3><a name="data"></a>Corpus Data</h3>
      <p>
        The NTI Buddhist Text Reader contains a small part-of-speech tagged corpus of Chinese Buddhist texts.
        The raw data, including word frequencies, can be found at the 
        <a href="https://github.com/alexamies/buddhist-dictionary">NTI Buddhist Text Reader GitHub Project</a> in the 
        data/dictionary folder. These are text files with tab delimiters.
        Examples will be used from the  
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/dictionary/unigram.txt">unigram.txt</a>
        file that lists word frequencies in the tagged corpus. 
        This is a tab delimited UTF-8 file with no header row.
        The structure of this file is:
      </p>
      <p class='formula'>
        <pre>
          pos_tagged_text: The element text with POS tag and gloss in pinyin and English
          element_text:    The element text in traditional Chinese
          word_id:         Matching id in the word table (positive integer)
          frequency:       The frequency of occurence of the word sense (positive integer)
        </pre>
      </p>
      <h3><a name="tools"></a>Software Tools</h3>
      <p>
        There are a number of software tools for statistical modelling and may be used for text analysis.
        Initially, R will be discussed.
      </p>
      <h4>R Project for Statistical Computing</h4>
      <p>
        The R Project for Statistical Computing (<a href="http://www.r-project.org/">http://www.r-project.org</a>),
        or simply R, is an open source project, language, and platform.
        You can freely download and install R for Linux, Mac, and Windows from links in the project web site.
        The R project web site has links to introductory materials but I used the book by Knell in preparing this
        content. (Knell,  <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and 
        Programming in R</i>)
      </p>
      <p>
        R is a generic statistics platform. In addition, there are a number of packages for R specifically for
        text analysis. These can be found in the web page 
        <a href="http://cran.r-project.org/web/packages/available_packages_by_name.html">Available CRAN Packages By Name</a>.
        Some examples are: 
        <a href="http://cran.r-project.org/web/packages/koRpus/index.html">koRpus: An R Package for Text Analysis</a>,
        <a href="http://cran.r-project.org/web/packages/tau/index.html">tau: Text Analysis Utilities</a>,
        <a href="http://cran.r-project.org/web/packages/textcat/index.html">textcat: N-Gram Based Text Categorization</a>,
        and <a href="http://cran.r-project.org/web/packages/tm/index.html">tm: Text Mining Package</a>.
      </p>
      <p>
        Let's look at some word frequencies with R. Here is a simple example of loading the frequencies from the 
        most common words in the table 
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/dictionary/unigram.txt">unigram.txt</a>.
        After installing R, open the command line interpreter with the command <code>R</code>.
        The unigram.txt file can be read in to a data frame using the <code>read.table()</code> function, as shown below.
        Change directories to the directory containing the file first.
      </p>
      <p class='formula'>
        <pre>
          $ R
          . . .
          &gt; names &lt;- c("pos_tagged_text", "element_text", "word_id", "frequency")
          &gt; unigram &lt;- read.table("unigram.txt", header=FALSE, sep="\t", quote="\"", col.names=names)
          &gt; head(unigram)
                       pos_tagged_text element_text word_id frequency
          1 須菩提/NR[xūpútí | Subhuti]        須菩提    6645       137
          2           是/PN[shì | this]           是   17908       136
          3             不/AD[bù | not]           不     502       130
          4          佛/NR[Fó | Buddha]           佛    3618       130
          5               於/P[yú | in]           於    1710        93
          6  如來/NR[Rúlái | Tathagata]          如來    6686        89
        </pre>
      </p>
      <p>
        The <code>c()</code> function contatenates the arguments into a vector, which is assigned to the 
        variable <code>names</code> using the assignment operator <code>&lt;-</code>.
        The variable <code>names</code> is used for the column names later.
        The unigram.txt file into the <code>unigram</code> data frame with the <code>read.table()</code> function.
        The <code>head</code> function prints out the first few lines of the data frame.
        The UTF-8 encoded Chinese characters are read in by R correctly.
        The NTI Reader text files are formatted to be loaded into MySQL and there are a few differences with the 
        basic form of the <code>read.table()</code> function.
        The format for <code>NULL</code> values in MySQL is <code>\N</code> but R expects <code>NA</code>.
        So, you will need to be careful if you depend on accurate representation of <code>NULL</code> values.
        In addition, the NTI Reader files do not have variable names in the first row.
      </p>
      <p>
        You can make a plot of data in R using the <code>plot()</code> function, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; x &lt;- seq(1, 1432)
          &gt; plot(x, unigram$frequency, xlab="", ylab="Frequency", type="n")
          &gt; points(x, unigram$frequency, pch=20, col="blue")
        </pre>
      </p>
      <p>
        The <code>seq()</code> function genrates a sequence of integers so that the frequency
        of each word will be plotted from most frequent at the left to least frequent at the right. 
        The <code>type="n"</code> parameter of the <code>plot()</code> function defers plotting of the points 
        to the next line where the <code>points()</code> function plots the points with symbol 20 and blue color.
        There are so many words that we cannot show the text for each one but it 
        is informative to see the general shape of the word frequency distribution.
        The image generated is shown in Figure 1.
      </p>
      <p class='picture'>
        <img src="images/unigram_frequency.png"/><br/>
        Figure 1: Word Frequency in the unigram Data Frame
      </p>
      <h3><a name="expectation"></a>Expectation</h2>
      <p>
        The <strong>expectation</strong> of a random variable is its <strong>mean</strong>.
        The expectation of a discrete random variable X with probability function f is defined as 
      </p>
      <p class='formula'>
        E(X) = Σ<sub>All x</sub> x f(x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 208)
      </p>
      <p>
        The <strong>variance</strong> of a discrete random variable X with mean μ is defined as
      </p>
      <p class='formula'>
        Var(X) = E[(X - μ)<sup>2</sup>]
      </p>
      <p>
        The <strong>standard deviation</strong> is the square root of the variance.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 226)
      </p>
      <p>
        The <strong>covariance</strong> of random variables X and Y with means μ<sub>x</sub> and μ<sub>y</sub> 
        is defined as
      </p>
      <p class='formula'>
        Cov(X, Y) = E[(X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)]
      </p>
      <p>
        assuming that the expectation exists.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 248)
      </p>
      <p>
        The <strong>correlation</strong> of random variables X and Y with variances σ<sub>x</sub><sup>2</sup> and 
        σ<sub>y</sub><sup>2</sup> is defined as
      </p>
      <p class='formula'>
        ρ(X, Y) = Cov(X, Y) / [σ<sub>x</sub>σ<sub>y</sub>]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 250)
      </p>
      <h3><a name="special"></a>Special Distributions</h2>
      <p>
        The <strong>Bernoulli distribution</strong> for random variable X, which can only take the values 0 and 1,
        with parameter p (0 ≤ p ≤ 1) has the probabilities
      </p>
      <p class='formula'>
        Pr(X = 1) = p and Pr(X = 0) = 1 - p
      </p>
      <p>
        An sequence random variables with the Bernoulli distribution are called <strong>Bernoulli trials</strong>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 276)
      </p>
      <p>
        The <strong>Binomial distribution</strong> with integer parameter n and continuous parameter p (0 ≤ p ≤ 1)
        is defined as 
      </p>
      <p class='formula'>
         f(x|n,p) = (n ¦ x) p<sup>x</sup>(1 - p)<sup>n-x</sup> for x = 0, 1, 2, ... and 0 otherwise
      </p>
      <p>
        where (n ¦ x) is the binomial coefficient n!/[x!(n - x)].
        The mean and variance are
      </p>
      <p class='formula'>
         E(X) = np
      </p>
      <p class='formula'>
         Var(X) = np(1 - p)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 277)
      </p>
      <p>
        The <strong>Poisson distribution</strong> for random variable X with mean λ is defined as
      </p>
      <p class='formula'>
         f(x|λ) = e<sup>-λ</sup> λ<sup>x</sup>/x! for x = 0, 1, 2, ... and 0 otherwise.
      </p>
      <p>
        The variance of the Poisson distribution is also λ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 288-290)
      </p>
      <p>
        The <strong>normal distribution</strong> for the continuous random variable X with mean μ and 
        standard deviation σ is defined as
      </p>
      <p class='formula'>
         f(x|μ, σ) = [1/σ√(2π)] exp[-0.5((x - μ)/σ)<sup>2</sup>] for -∞ &lt; x &lt; ∞
      </p>
      <p>
        The standard normal distribution has mean μ = 0 and standard deviation σ = 1.
        It can be plotted using the <code>curve()</code> function in R, as shown below.
      <p class='formula'>
        <pre>
          &gt; curve(exp(-x^2)/(2*sqrt(pi)), -3, 3, xlab="z", ylab="Probability Density")
        </pre>
      </p>
      <p>
        The graph generated is shown in Figure 2.
      </p>
      <p class='picture'>
        <img src="images/standard_normal.png"/><br/>
        Figure 2: Standard Normal Distribution
      </p>
      <p>
        The normal distribution is a good approximation for variables in many random processes.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 303)
        The <strong>Central Limit Theorem</strong> states that the distribution of a sum of 
        random variables Σ<sub>i=1</sub><sup>n</sup> X<sub>i</sub>  with any distribution will be
        approximately the normal distribition with mean nμ and variance nσ<sup>2</sup>, as
        n becomes large.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 361)
      </p>
      <p>
        The <strong>gamma distribution</strong> for the continuous random variable X with parameters 
        α and β is 
      </p>
      <p class='formula'>
         f(x|α, β) = [β<sup>α</sup> / Γ(α)] x<sup>α-1</sup> e<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        where Γ(α) is the gamma function. 
        The mean of the gamma distribution is α/β and the variance is α/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 319-320)
      </p>
      <p>
        The <strong>exponential distribution</strong> for the continuous random variable X with parameter β is
      </p>
      <p class='formula'>
         f(x|β) = βe<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        The exponential distribution is a special case of the gamma distribution with α = 1.
        The mean of the exponential distribution is 1/β and the variance is 1/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 321)
      </p>
      <p>
        The <strong>beta distribution</strong> for the continuous random variable X with parameters α and β is
      </p>
      <p class='formula'>
         f(x|α, β) = [Γ<sup>β+α</sup> / (Γ(α) Γ(β))] x<sup>α-1</sup> (1 - x)<sup>β-1</sup> for 0 &lt; x &lt; 1 or 0 otherwise
      </p>
      <p>
        The mean of the beta distribution is 
      </p>
      <p class='formula'>
        E(X) = α/(α + β)
      </p>
      <p>
        The variance of the beta distribution is 
      </p>
      <p class='formula'>
        Var(X) = αβ/[(α + β)<sup>2</sup>(α + β + 1)]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 328-329)
      </p>
      <p>
        The <strong>multinomial distribution</strong> for the discrete random vector 
        <strong>X</strong> = (X<sub>1</sub>, X<sub>2</sub>, ... X<sub>k</sub>) having probabilities
        <strong>p</strong> = (p<sub>1</sub>, p<sub>2</sub>, ... p<sub>k</sub>) 
        with n items selected is defined as
      </p>
      <p class='formula'>
         f(<strong>X</strong>|n, <strong>p</strong>) = [n!/(x<sub>1</sub> x<sub>2</sub> ... x<sub>k</sub>)]
         p<sub>1</sub><sup>x<sub>1</sub></sup> p<sub>2</sub><sup>x<sub>2</sub></sup> ... p<sub>k</sub><sup>x<sub>k</sub></sup>
      </p>
      <p>
        if x<sub>1</sub> + x<sub>2</sub> + ... x<sub>k</sub> = n or 0 otherwise.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 334)
        The multinomial distribution is appropriate for distributions into sets that are not necessarily numbers,
        for example, the frequencies of words of different part of speech values.
      </p>
      <h3><a name="estimation"></a>Statistical Estimation</h3>
      <p>
        A <strong>statistical model</strong> is a collection of random variables, identification of probability
        distributions for the variables, and the set of parameters that the distributions require values for.
        <strong>Statistical inference</strong> is a probabilistic statement about a statistical model.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 377-378)
        For example, the approximate date of a document may be inferred from the vocabulary in it.
        (Krippendorff, <i>Content Analysis</i>, 42)
        In a Buddhist text mention of copying sutras or description of devotional practices may help provide information for the date
        for the text. The data may be recorded as 0 (not present) and 1 (present) or as a word frequency.
      </p>
      <p>
        A <strong>statistic</strong> is a function of a set of random variables. For example, mean, median, 
        and variance are statistics.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 382)
      </p>
      <p>
        Parameters in probability distributions are usually unknown and needed to be estimated with statistical methods.
        So the parameters themselves can be considered simply as unknown constants or to have probability distributions
        themselves.
        The <strong>prior distribution</strong> of a parameter θ is the probability distribution ζ(θ) assumed before
        experimental observations are applied to estimate its value.
        The <strong>posterior distribution</strong> ζ(θ|x<sub>1</sub>, ... x<sub>n</sub>) 
        is the conditional distribution after the random variables X<sub>1</sub>, ... X<sub>n</sub> have been observed.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 385-387)
        The <strong>likelihood function</strong> f<sub>n</sub>(<strong>x</strong>|θ) is the joint probability function
        of the random variables <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>) and the parameter θ.
        The likelihood function can be used to relate teh prior and posterior distributions,
      </p>
      <p class='formula'>
         ζ(θ|<strong>x</strong>) ∝ f<sub>n</sub>(<strong>x</strong>|θ) ζ(θ)
      </p>
      <p>
        The constant of proportionality can be found from equating the total probability to 1.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 390)
      </p>
      <p>
        A <strong>conjugate family of prior distributions</strong> is a family of possible distributions for ζ(θ) where 
        the posterior distribution also belongs to the same family. 
        The family of gamma distributions is a conjugate family of prior distributions for Poisson distributions of
        the random variables X<sub>1</sub>, ... X<sub>n</sub> when the parameter θ is unknown.
        The family of normal distributions is itself a conjugate family of prior distributions for normal distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the mean is unknown but the variance is known.
        The family of gamma distributions is also a conjugate family of prior distributions for exponential distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the value of the parameter θ is unknown.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 395-402)
      </p>
      <p>
        An <strong>estimator</strong> δ(X<sub>1</sub>, ... X<sub>n</sub>) gives an estimate of the parameter θ using
        observed values of the data <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>). 
        A <strong>loss function</strong> L(θ, a) quantifies the effect of the difference between the estimate a of θ
        and the true value.
        A <strong>Bayes estimator</strong> δ*(<strong>x</strong>) minimizes the expected value of the loss function.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 408-409)
      </p>
      <p>
        The squared error loss function is defined as 
      </p>
      <p class='formula'>
         L(θ, a) = (θ - a)<sup>2</sup>
      </p>
      <p>
        When the squared error loss function is used the Bayes estimator is the posterior mean value of θ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 411)
      </p>
      <h3><a name="references"></a>References</h3>
      <ol>
        <li>
          Amies, Alex. NTI Buddhist Text Reader GitHub Project, 2014. https://github.com/alexamies/buddhist-dictionary.
        </li>
        <li>
          DeGroot, Morris H., and Mark J. Schervish. <i>Probability and Statistics</i>. 4 edition. Boston: Pearson, 2011.
        </li>
        <li>
          Knell, Robert. <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and Programming in R</i>. United Kingdom: Self published, 2013.
        </li>
        <li>
          Krippendorff, Klaus H. <i>Content Analysis: An Introduction to Its Methodology</i>. Third Edition edition. SAGE Publications, Inc, 2012.
        </li>
        <li>
          The R Project for Statistical Computing (version Pumpkin Helmet). R Foundation, 2014. 
          <a href="http://www.r-project.org/">http://www.r-project.org</a>.
        </li>
      <hr/>
      <p>
        Copyright Nan Tien Institute 2013 - 2014, 
        <a href="http://www.nantien.edu.au/" title="Fo Guang Shan Nan Tien Institute">www.nantien.edu.au</a>.
      </p>
      <p>This page was last updated on November 8, 2014.</p>
     </div>
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script>
  </body>
</<html>
