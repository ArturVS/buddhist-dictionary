<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="NTI Buddhist Text Reader">
    <title>NTI Buddhist Text Reader</title>
    <link rel="shortcut icon" href="images/yan.png" type="image/jpeg" />
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="buddhistdict.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="starter-template">
      <div class="row">
        <div class="span2"><img id="logo" src="images/yan.png" alt="Logo" class="pull-left"/></div>
        <div class="span7"><h1>NTI Buddhist Text Reader</h1></div>
      </div>
    </div>
    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Home</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="corpus.html">Texts</a></li>
            <li class="active"><a href="tools.html">Tools</a></li>
            <li><a href="dict_resources.html">Resources</a></li>
            <li><a href="about.html">About</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
      <h2>Statistical Methods</h2>
      <h3>Contents</h3>
      <p>
        <ul>
          <li><a href="#data">Corpus Data</a></li>
          <li><a href="#tools">Software Tools</a></li>
          <li><a href="#probability">Probability</a></li>
          <li><a href="#expectation">Expectation</a></li>
          <li><a href="#special">Special Distributions</a></li>
          <li><a href="#estimation">Statistical Estimation</a></li>
          <li><a href="linear">Linear Regression</a></li>
          <li><a href="#references">References</a></li>
        </ul>
      </p>
      <p>
        This page gives some incomplete notes on statistical methods in analyzing Chinese text 
        and analyzing quality of corpus data.
      </p>
      <h3><a name="data"></a>Corpus Data</h3>
      <p>
        A <strong>corpus</strong> is a collection of texts, which may be used by linguists for studying the 
        characteristics of language usage. (Bird, <i>Natural Language Processing</i>, 39)
        The NTI Buddhist Text Reader contains a corpus of Buddhist and classical Chinese texts.
        A small subset of the corpus is tagged part-of-speech taggs and gloss distinguishing word sense.
        The organization of the corpus is described on the page
        <a href="annotation.html">Text Management, Annotation, and Gloss</a> on this web site.
        The tagging scheme is described on the page <a href="pos_tags.html">Part-of-Speech Tag Definitions</a>.
        The raw data, including word frequencies, can be found at the 
        <a href="https://github.com/alexamies/buddhist-dictionary">NTI Buddhist Text Reader GitHub Project</a> in the 
        data/dictionary folder. They are tab delimited text files.
      </p>
      <p>
        Word frequencies from the tagged subset of the corpus can found in the files
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/dictionary/unigram.txt">unigram.txt</a>
        and 
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/dictionary/bigram.txt">bigram.txt</a>
        These are a tab delimited UTF-8 file with no header row.
        The unigram.txt file lists single word frequencies in the tagged corpus. 
        The structure of the unigram.txt file is:
      </p>
      <p class='formula'>
        <pre>
          pos_tagged_text: The element text with POS tag and gloss in pinyin and English
          element_text:    The element text in traditional Chinese
          word_id:         Matching id in the word table (positive integer)
          frequency:       The frequency of occurence of the word sense (positive integer)
        </pre>
      </p>
      <p>
        The bigram.txt file lists frequencies of two-word combinations.
        The structure of the bigram.txt file is:
      </p>
      <p class='formula'>
        <pre>
          pos_tagged_text: The element text with POS tag and gloss in pinyin and English
          previous_text:   The element text in traditional Chinese
          element_text:    The element text in traditional Chinese
          word_id:         Matching id in the word table (positive integer)
          frequency:       The frequency of occurence of the word sense (positive integer)
        </pre>
      </p>
      <p>
        A tab delimited plain text list of summary statistcs for the corpus texts is included in the file 
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/stats/corpus_stats.txt">corpus_stats.txt</a>. 
        The structure of the file is 
      </p>
      <p class='formula'>
        <pre>
          source_name: the title of the text
          word_count: the number of words in the text
          character_count: the number of characters in the text
          unique_words: the number of unique words in the text
        </pre>
      </p>
      <h3><a name="tools"></a>Software Tools</h3>
      <p>
        There are a number of software tools for statistical modelling and may be used for text analysis.
        Initially, R will be discussed.
      </p>
      <h4>R Project for Statistical Computing</h4>
      <p>
        The R Project for Statistical Computing (<a href="http://www.r-project.org/">http://www.r-project.org</a>),
        or simply R, is an open source project, language, and platform.
        You can freely download and install R for Linux, Mac, and Windows from links in the project web site.
        The R project web site has links to introductory materials but I used the book by Knell in preparing this
        content. (Knell,  <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and 
        Programming in R</i>)
      </p>
      <p>
        R is a generic statistics platform. In addition, there are a number of packages for R specifically for
        text analysis. These can be found in the web page 
        <a href="http://cran.r-project.org/web/packages/available_packages_by_name.html">Available CRAN Packages By Name</a>.
        Some examples are: 
        <a href="http://cran.r-project.org/web/packages/koRpus/index.html">koRpus: An R Package for Text Analysis</a>,
        <a href="http://cran.r-project.org/web/packages/tau/index.html">tau: Text Analysis Utilities</a>,
        <a href="http://cran.r-project.org/web/packages/textcat/index.html">textcat: N-Gram Based Text Categorization</a>,
        and <a href="http://cran.r-project.org/web/packages/tm/index.html">tm: Text Mining Package</a>.
      </p>
      <p>
        After installing R, open the command line interpreter with the command <code>R</code>.
        The unigram.txt file can be read in to a data frame using the <code>read.table()</code> function, as shown below.
        Change directories to the directory containing the file first.
      </p>
      <p class='formula'>
        <pre>
          $ R
          . . .
          &gt; names &lt;- c("pos_tagged_text", "element_text", "word_id", "frequency")
          &gt; unigram &lt;- read.table("unigram.txt", header=FALSE, sep="\t", quote="\"", col.names=names, numerals ="allow.loss")
          &gt; head(unigram)
                       pos_tagged_text element_text word_id frequency
          1 須菩提/NR[xūpútí | Subhuti]        須菩提    6645       137
          2           是/PN[shì | this]           是   17908       136
          3             不/AD[bù | not]           不     502       130
          4          佛/NR[Fó | Buddha]           佛    3618       130
          5               於/P[yú | in]           於    1710        93
          6  如來/NR[Rúlái | Tathagata]          如來    6686        89
        </pre>
      </p>
      <p>
        The <code>$</code> prompt is shown before a shell command and the <code>&gt;</code> prompt is shown 
        before an R command.
        The <code>c()</code> function contatenates the arguments into a vector, which is assigned to the 
        variable <code>names</code> using the assignment operator <code>&lt;-</code>.
        The variable <code>names</code> is used for the column names later.
        The unigram.txt file into the <code>unigram</code> data frame with the <code>read.table()</code> function.
        The <code>head</code> function prints out the first few lines of the data frame.
        The UTF-8 encoded Chinese characters are read in by R correctly.
        The NTI Reader text files are formatted to be loaded into MySQL and there are a few differences with the 
        basic form of the <code>read.table()</code> function.
        The format for <code>NULL</code> values in MySQL is <code>\N</code> but R expects <code>NA</code>.
        So, you will need to be careful if you depend on accurate representation of <code>NULL</code> values.
        In addition, the NTI Reader files do not have variable names in the first row.
      </p>
      <p>
        The bigram.txt file can be read in to a data frame in a similar way, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; binames &lt;- c("pos_tagged_text", "previous_text", "element_text", "word_id", "frequency")
          &gt; bigram &lt;- read.table("bigram.txt", header=FALSE, sep="\t", quote="\"", col.names=binames, numerals ="allow.loss")
          &gt; head(bigram)
                 pos_tagged_text previous_text element_text word_id frequency
          1   故/NN[gù | purpose]            以           故    7115        38
          2    以/P[yǐ | because]            何           以   30648        38
          3 云何/DT[yúnhé | what]            意         云何   29319        32
          4      意/NN[yì | idea]            於           意    1730        30
          5    佛/NR[Fó | Buddha]            諸           佛    3618        26
          6  說/VV[shuō | speaks]          如來           說     412        23

        </pre>
      </p>
      <p>
        You can make a plot of data in R using the <code>plot()</code> function, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; x &lt;- seq(1, 1432)
          &gt; plot(x, unigram$frequency, xlab="", ylab="Frequency", type="n")
          &gt; points(x, unigram$frequency, pch=20, col="blue")
        </pre>
      </p>
      <p>
        The <code>seq()</code> function genrates a sequence of integers so that the frequency
        of each word will be plotted from most frequent at the left to least frequent at the right. 
        The <code>type="n"</code> parameter of the <code>plot()</code> function defers plotting of the points 
        to the next line where the <code>points()</code> function plots the points with symbol 20 and blue color.
        There are so many words that we cannot show the text for each one but it 
        is informative to see the general shape of the word frequency distribution.
        The image generated is shown in Figure 1.
      </p>
      <p class='picture'>
        <img src="images/unigram_frequency.png"/><br/>
        Figure 1: Word Frequency in the unigram Data Frame
      </p>
      <p>
        This is a useful diagram because it shows the breadth and depth of the corpus at a glance. The corpus
        only has about 1,400 unique words and only has a substantial frequency for less than 200 of these words.
      </p>
      <p>
        You will need to install the R <code>showtext</code> package to properly display Chinese text on R generated graphics.
        Use the commands below to install <code>showtext</code>.
      </p>
      <p class='formula'>
        <pre>
          &gt; install.packages("showtext")
          &gt; library(showtext)
        </pre>
      <p>
        You can make a histogram plot of the frequency data using the <code>barplot()</code> function, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; freq &lt;- as.vector(as.matrix(unigram[4])[1:8, 1])
          &gt; freq.labels &lt;- as.vector(as.matrix(unigram[2])[1:8, 1])
          &gt; png(filename="ungram_barplot.png", width = 480, height = 480)
          &gt; showtext.begin()
          &gt; barplot(freq, names.arg=freq.labels, ylab="Frequency", col="steelblue")
          &gt; showtext.end()
          &gt; dev.off()
        </pre>
      </p>
      <p>
        A subset of fourth column of the unigram data.frame is read into the variable <code>freq</code>, after being converted to a vector.
        The labels are the Chinese text for each word taken from column 2.
        Rather than generate the graph in a window, the code above writes it to a png file.
        The <code>showtext.begin()</code> needs to be called before generating the graph.
        When <code>dev.off()</code> is called the file will be written.
        The chart generated is shown in Figure 2.
      </p>
      <p class='picture'>
        <img src="images/ungram_barplot.png"/><br/>
        Figure 2: Word Frequency Bar Plot for a Subset of the Data
      </p>
      <h3><a name="probability"></a>Probability</h2>
      <p>
        <strong>Example</strong>: Different Chinese characters have acquired multiple meanings over history.
        This makes choosing the appropriate word meaning require understanding of the context.
        One of the goals of the NTI Reader is to help the user decide which is the most appropriate word sense.
        Suppose that we wish to find the probability of choosing the right meaning of the character 法 (fǎ).
        法 (fǎ) most often means the noun "law" or the noun "method" in modern Chinese. 
        However, in Buddhist texts written in literary Chinese, 法 might mean the proper noun "Dharma" 
        (the teachings of the Buddha), teachings in general, or the noun "dharma" (phenomenon).
        The word frequencies for each sense can be found from the unigram table using the following
        R commands.
      </p>
      <p class='formula'>
        <pre>
          &gt; fa &lt;- subset(unigram, element_text=="法")
          &gt; fa
          pos_tagged_text                 element_text word_id frequency
          10          法/NN[fǎ | a dhárma]           法   17994        74
          1253          法/NR[Fǎ | Dhárma]           法    3509         1
          1376 法/NN[fǎ | a mental object]           法   32204         1
          1397          法/NN[fǎ | method]           法    3506         1
          &gt; PrA = fa$frequency[1] / sum(fa$frequency)
          &gt; PrA
          [1] 0.961039
        </pre>
      </p>
      <p>
        The <code>subset()</code> function selects a subset of the unigram table with the value of element_text equal to "法".
        The total number of occurences of 法 in the tagged corpus is <code>sum(fa$frequency) = 74 + 1 + 1 + 1 = 77</code>.
        So the probability of any one occurence of 法 being the noun (NN) meaning "a dhárma" (a teaching) is 
        <code>74/77 = 0.961</code>. 
        If we had no other information about the context of a occurence of 法, then we would guess that it would mean
        a dhárma. ▢
      </p>
      <p>
        The <strong>conditional probability</strong> of an event A given that we know an event B has already occured is written Pr(A|B).
        It can be computed as 
      </p>
      <p class='formula'>
        Pr(A|B) = Pr(A ∩ B) / Pr(B)
      </p>
      <p>
        This assumes that Pr(B) &gt; 0. (DeGroot and Morris, <i>Probability and Statistics</i>, 56)
      </p>
      <p>
        <strong>Example</strong>: If we know the word before 法, it may give us a better chance at picking the correct
        word sense. This is an example of conditional probability
        We can use the bigram table to compute the conditinal probability of the word sense of 法.
        Suppose the word before is 說 shuō "to say" in modern Chinese but more commonly "to teach" in 
        literary Chinese.
        That would make the phrase be 說法 "to teach a dhárma."
        Let A = the proper meaning of 法 in this instance is 法/NN[fǎ | a dhárma].
        Let B = the word before 法 is 說.
        The conditional probability can be computed using the R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; subset(subset(bigram, element_text=="法"), previous_text=="說")
                 pos_tagged_text previous_text element_text word_id frequency
          34 法/NN[fǎ | a dhárma]            說           法   17994         9
        </pre>
      </p>
      <p class='formula'>
        Pr(A|B) = 9 / 9 = 1.0
      </p>
      <p>
        That is, whenever the word before 法 is 說 then the proper word sense is always 法/NN[fǎ | a dhárma].
        The interpretation of this is that the previous word is a very good predictor of word sense,
        in this case. ▢
      </p>
      <p>
        Two events A and B are <strong>independent</strong> if the occurence of one does not affect the occurrence of the other. 
        If this is true then
      </p>
      <p class='formula'>
        Pr(A ∩ B) = Pr(A) Pr(B)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 66)
      </p>
      <p>
        A <strong>random variable</strong> is a real-valued function in a sample space S. (DeGroot and Morris, <i>Probability and Statistics</i>, 93)
      </p>
      <p>
        The <strong>probability function</strong> of a discrete random variable X is the function
      </p>
      <p class='formula'>
        f(x) = Pr(X = x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 96)
        In linguistics discrete random variables, like word frequency, are more common but continuous random
        variables may also be used, for example the word frequency of a specific word per 1,000 words of text.
        The equivalent function for a continuous random variable is called the 
        <strong>probability density function</strong> (pdf).
      </p>
      <p>
        The <strong>cummulative distribution function</strong> (cdf) of a random variable X is 
      </p>
      <p class='formula'>
        F(x) = Pr(X ≤ x) for -∞ &lt; x &lt; ∞
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 108)
      </p>
      <p>
        The <strong>quantile function</strong> F<sup>-1</sup>(p) of a random variable X is the inverse of the cdf,
        which is also the smallest value x  with F(x) ≥ p. The variable p is the probability.
        F<sup>-1</sup>(p) is the p quantile of X or 100p percentile.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 112)
        A <strong>quartile</strong> is found by sorting the data and then dividing it into four equal groups.
        The <strong>interquartile range</strong> is the middle two quartiles or, in other words, the range 
        between the 25 and 75th percentiles.
        Quartile and range information can be found using the R function <code>summary()</code>.
        The quantile can be found using the R function <code>qt()</code>.
        The interquartile range can be found using the R function <code>IQR()</code>.
      </p>
      <p>
        <strong>Example</strong>: 
        Summary data for the word frequency data can be found as follows.
      </p>
      <p class='formula'>
        <pre>
          &gt; summary(unigram$frequency)
          Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
          1.000   1.000   1.000   4.708   3.000 137.000 
          &gt; length(unigram$frequency)
          [1] 1432
        </pre>
      </p>
      <p>
        The <code>length()</code> function gives the number of items in the data set, which shows that there
        are only 1,432 unique words in the tagged corpus. ▢
      </p>
      <p>
        The <strong>joint probability function</strong> of two random variables X and Y is 
      </p>
      <p class='formula'>
        f(x, y) = Pr(X = x and Y = y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 119)
      </p>
      <p>
        The <strong>marginal cdf</strong> of a joint probability function of two discrete random variables X and Y is 
        summed over all possible values of y. In symbols,
      </p>
      <p class='formula'>
        f<sub>1</sub>(x) = Σ<sub>All y</sub> f(x, y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 131)
      </p>
      <p>
        A <strong>stochastic process</strong> is a sequence of random variables X<sub>1</sub>, X<sub>2</sub>, ... at discrete points
        in time. A <strong>Markov chain</strong> is a stochastic process where the conditional distributions of all X<sub>n+j</sub>
        depend only on X<sub>n</sub> and not only earlier states.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 188)
      </p>
      <p>
        The <strong>transition distributions</strong> of a Markov chain are the conditional probabilities 
      </p>
      <p class='formula'>
        p<sub>ij</sub> = Pr(X<sub>n+1</sub>=j|X<sub>n</sub>=i)
      </p>
      <p>
        where the random variables X<sub>n</sub> can have k possible states.
        A <strong>transition matrix</strong> is a matrix <strong>P</strong> = [p<sub>ij</sub>] made up of the conditional probabilities of the 
        transition distributions.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 190-192)
      </p>
      <p>
        <strong>Example</strong>: A stream of words can be thought of as a Markov chain. The earlier words can influence the later
        words. In a simple statistical model, each word may only be influenced by the preceding words. 
      </p>
      <h3><a name="expectation"></a>Expectation</h2>
      <p>
        The <strong>expectation</strong> or of a random variable is its <strong>mean</strong>.
        The expectation of a discrete random variable X with probability function f is defined as 
      </p>
      <p class='formula'>
        E(X) = Σ<sub>All x</sub> x f(x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 208)
        The <strong>median</strong> is another measure of central tendency, which separates the lower half from
        the upper half of a set of numbers. It can be more useful than the mean
        when dealing with small integers or highly skewed data.
        The mean of a vector of numbers can be found with the R function <code>mean()</code> and the median 
        can be found with the function <code>median()</code>.
      </p>
      <p>
        <strong>Example</strong>: 
        The values of mean and median for word frequency in the NTI Reader tagged corpus can be found with the 
        R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; mean(unigram$frequency)
          [1] 4.708101
          &gt; median(unigram$frequency)
          [1] 1
        </pre>
      </p>
      <p>
        This can give some idea of the adequacy of the size of the tagged corpus.
        A mean frequency of about 4.7 word occurrences and a median of 1 in the tagged corpus makes the tagged corpus
        seem kind of small. We need to do more analysis to understand what might be really sufficient. ▢
      </p>
      <p>
        The <strong>variance</strong> of a discrete random variable X with mean μ is defined as
      </p>
      <p class='formula'>
        Var(X) = E[(X - μ)<sup>2</sup>]
      </p>
      <p>
        The <strong>standard deviation</strong> is the square root of the variance.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 226)
        The <strong>sample variance</strong> is the variance of a sample, correcting for the fact that it is 
        a sample of a larger population.
        The sample variance is given by the formula
      </p>
      <p class='formula'>
        s<sub>x</sub><sup>2</sup> = Σ(x - x̄)<sup>2</sup>/(n - 1)
      </p>
      <p>
        where x̄ is the sample mean and n - 1 is the degrees of freedom.
        s<sub>x</sub> is the sample standard deviation.
        The sample variance can be computed fwith the R function <code>var()</code> and the sample standard
        deviation with the function <code>sd()</code>.
        (Knell, <i>Introductory R</i>, 142)
      </p>
      <p>
        <strong>Example</strong>: 
        The variance and standard deviation of word frequency in the NTI Reader tagged corpus can be found with the 
        R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; v &lt;- var(unigram$frequency)
          &gt; v
          [1] 140.1174
          &gt; sqrt(v)
         [1] 11.83712
        </pre>
      </p>
      <p>
        The variance is about 140.1 and the standard deviation about 11.8. ▢
      </p>
      <p>
        The <strong>covariance</strong> of random variables X and Y with means μ<sub>x</sub> and μ<sub>y</sub> 
        is defined as
      </p>
      <p class='formula'>
        Cov(X, Y) = E[(X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)]
      </p>
      <p>
        assuming that the expectation exists.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 248)
        The <strong>sample covariance</strong> corrects for sampling from a larger population. 
        It is given by the formula:
      </p>
      <p class='formula'>
        Cov<sub>x, y</sub> = Σ(x - x̄)(y - ȳ)/(n - 1)
      </p>
      <p>
        where x̄ and ȳ are the sample means and n - 1 is the degrees of freedom.
        (Knell, <i>Introductory R</i>, 211)
        The sample covariance can be computed using the R function <code>cov()</code>.
      </p>
      <p>
        The <strong>correlation</strong> of random variables X and Y with variances σ<sub>x</sub><sup>2</sup> and 
        σ<sub>y</sub><sup>2</sup> is defined as
      </p>
      <p class='formula'>
        ρ(X, Y) = Cov(X, Y) / [σ<sub>x</sub>σ<sub>y</sub>]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 250)
        The correlation for a sample can be computed from the formula
      </p>
      <p class='formula'>
        r = Σ(x - x̄)(y - ȳ)/[(n - 1)(s<sub>x</sub> s<sub>y</sub>)]
      </p>
      <p>
        where s<sub>x</sub> and s<sub>y</sub> are the sample standard deviations.
        (Knell, <i>Introductory R</i>, 212)
        The sample correlation can be computed using the R function <code>cor()</code>.
      </p>
      <h3><a name="special"></a>Special Distributions</h2>
      <p>
        The <strong>Bernoulli distribution</strong> for random variable X, which can only take the values 0 and 1,
        with parameter p (0 ≤ p ≤ 1) has the probabilities
      </p>
      <p class='formula'>
        Pr(X = 1) = p and Pr(X = 0) = 1 - p
      </p>
      <p>
        An sequence random variables with the Bernoulli distribution are called <strong>Bernoulli trials</strong>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 276)
      </p>
      <p>
        The <strong>Binomial distribution</strong> with integer parameter n and continuous parameter p (0 ≤ p ≤ 1)
        is defined as 
      </p>
      <p class='formula'>
         f(x|n,p) = (n ¦ x) p<sup>x</sup>(1 - p)<sup>n-x</sup> for x = 0, 1, 2, ... and 0 otherwise
      </p>
      <p>
        where (n ¦ x) is the binomial coefficient n!/[x!(n - x)].
        The mean and variance are
      </p>
      <p class='formula'>
         E(X) = np
      </p>
      <p class='formula'>
         Var(X) = np(1 - p)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 277)
        The R function to compute the binomial probability is <code>dbinom()</code>.
        The commands to plot a binomial distribution with parameters n = 20 and p = 0.5 are shown below.
      </p>
      <p class='formula'>
        <pre>
         &gt; x &lt;- seq(1, 20)
         &gt; png(filename="binomial05.png", width = 400, height = 400)
         &gt; plot(x, dbinom(x, 20, 0.5), xlab="x", ylab="Frequency", pch=17, col="blue")
         &gt; dev.off()
        </pre>
      </p>
      <p>
        The <code>plot()</code> function takes a sequence of integers <code>x</code> from 1 to 20
        and plots the binomial probability for each value.
        The graph generated is shown in Figure 3.
      </p>
      <p class='picture'>
        <img src="images/binomial05.png"/><br/>
        Figure 3: Binomial Distribution with Parameters n = 20 and p = 0.5
      </p>
      <p>
        The <strong>Poisson distribution</strong> for random variable X with mean λ is defined as
      </p>
      <p class='formula'>
         f(x|λ) = e<sup>-λ</sup> λ<sup>x</sup>/x! for x = 0, 1, 2, ... and 0 otherwise.
      </p>
      <p>
        The variance of the Poisson distribution is also λ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 288-290)
        The <code>dpois()</code> R function can be used to compute values of the Poisson distribution.
        This is demonstrated with the R commands below, which generate a graph of the 
        Poisson distribution with λ = 3.0.
      </p>
      <p class='formula'>
        <pre>
         &gt; x &lt;- seq(1, 10)
         &gt; png(filename="poisson_lambda3.png", width = 400, height = 400)
         &gt; plot(x, dpois(x, 3.0), xlab="x", ylab="Frequency", pch=17, col="blue")
         &gt; dev.off()
        </pre>
      </p>
      <p>
        The graph generated is shown in Figure 4.
      </p>
      <p class='picture'>
        <img src="images/poisson_lambda3.png"/><br/>
        Figure 4: Poisson Distribution with λ = 3.0
      </p>
      <p>
        The <strong>normal distribution</strong> for the continuous random variable X with mean μ and 
        standard deviation σ is defined as
      </p>
      <p class='formula'>
         f(x|μ, σ) = [1/σ√(2π)] exp[-0.5((x - μ)/σ)<sup>2</sup>] for -∞ &lt; x &lt; ∞
      </p>
      <p>
        The standard normal distribution has mean μ = 0 and standard deviation σ = 1.
        It can be plotted using the <code>curve()</code> function in R, as shown below.
      <p class='formula'>
        <pre>
          &gt; curve(exp(-x^2)/(2*sqrt(pi)), -3, 3, xlab="z", ylab="Probability Density")
        </pre>
      </p>
      <p>
        The graph generated is shown in Figure 5.
      </p>
      <p class='picture'>
        <img src="images/standard_normal.png"/><br/>
        Figure 5: Standard Normal Distribution
      </p>
      <p>
        The normal distribution is a good approximation for variables in many random processes.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 303)
        The <strong>Central Limit Theorem</strong> states that the distribution of a sum of 
        random variables Σ<sub>i=1</sub><sup>n</sup> X<sub>i</sub>  with any distribution will be
        approximately the normal distribition with mean nμ and variance nσ<sup>2</sup>, as
        n becomes large.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 361)
      </p>
      <p>
        The <strong>gamma distribution</strong> for the continuous random variable X with parameters 
        α and β is 
      </p>
      <p class='formula'>
         f(x|α, β) = [β<sup>α</sup> / Γ(α)] x<sup>α-1</sup> e<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        where Γ(α) is the gamma function. 
        The mean of the gamma distribution is α/β and the variance is α/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 319-320)
      </p>
      <p>
        The <strong>exponential distribution</strong> for the continuous random variable X with parameter β is
      </p>
      <p class='formula'>
         f(x|β) = βe<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        The exponential distribution is a special case of the gamma distribution with α = 1.
        The mean of the exponential distribution is 1/β and the variance is 1/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 321)
        The exponential distribution can be drawn with the R command below.
      </p>
      <p class='formula'>
        <pre>
          &gt; curve(0.8*exp(-0.8*x), 0, 5, xlab="x", ylab="Probability Density", col="blue")
        </pre>
      </p>
      <p>
        The generated graph is shown in Figure 6.
      </p>
      <p class='picture'>
        <img src="images/exponential_distribution08.png"/><br/>
        Figure 6: Exponential Distribution (β = 0.8)
      </p>
      <p>
        The <strong>beta distribution</strong> for the continuous random variable X with parameters α and β is
      </p>
      <p class='formula'>
         f(x|α, β) = [Γ<sup>β+α</sup> / (Γ(α) Γ(β))] x<sup>α-1</sup> (1 - x)<sup>β-1</sup> for 0 &lt; x &lt; 1 or 0 otherwise
      </p>
      <p>
        The mean of the beta distribution is 
      </p>
      <p class='formula'>
        E(X) = α/(α + β)
      </p>
      <p>
        The variance of the beta distribution is 
      </p>
      <p class='formula'>
        Var(X) = αβ/[(α + β)<sup>2</sup>(α + β + 1)]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 328-329)
      </p>
      <p>
        The <strong>multinomial distribution</strong> for the discrete random vector 
        <strong>X</strong> = (X<sub>1</sub>, X<sub>2</sub>, ... X<sub>k</sub>) having probabilities
        <strong>p</strong> = (p<sub>1</sub>, p<sub>2</sub>, ... p<sub>k</sub>) 
        with n items selected is defined as
      </p>
      <p class='formula'>
         f(<strong>X</strong>|n, <strong>p</strong>) = [n!/(x<sub>1</sub> x<sub>2</sub> ... x<sub>k</sub>)]
         p<sub>1</sub><sup>x<sub>1</sub></sup> p<sub>2</sub><sup>x<sub>2</sub></sup> ... p<sub>k</sub><sup>x<sub>k</sub></sup>
      </p>
      <p>
        if x<sub>1</sub> + x<sub>2</sub> + ... x<sub>k</sub> = n or 0 otherwise.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 334)
        The multinomial distribution is appropriate for distributions into sets that are not necessarily numbers,
        for example, the frequencies of words of different part of speech values.
      </p>
      <h3><a name="estimation"></a>Statistical Estimation</h3>
      <p>
        A <strong>statistical model</strong> is a collection of random variables, identification of probability
        distributions for the variables, and the set of parameters that the distributions require values for.
        <strong>Statistical inference</strong> is a probabilistic statement about a statistical model.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 377-378)
        For example, the approximate date of a document may be inferred from the vocabulary in it.
        (Krippendorff, <i>Content Analysis</i>, 42)
        In a Buddhist text mention of copying sutras or description of devotional practices may help provide information for the date
        for the text. The data may be recorded as 0 (not present) and 1 (present) or as a word frequency.
      </p>
      <p>
        A <strong>statistic</strong> is a function of a set of random variables. For example, mean, median, 
        and variance are statistics.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 382)
      </p>
      <p>
        Parameters in probability distributions are usually unknown and needed to be estimated with statistical methods.
        So the parameters themselves can be considered simply as unknown constants or to have probability distributions
        themselves.
        The <strong>prior distribution</strong> of a parameter θ is the probability distribution ζ(θ) assumed before
        experimental observations are applied to estimate its value.
        The <strong>posterior distribution</strong> ζ(θ|x<sub>1</sub>, ... x<sub>n</sub>) 
        is the conditional distribution after the random variables X<sub>1</sub>, ... X<sub>n</sub> have been observed.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 385-387)
        The <strong>likelihood function</strong> f<sub>n</sub>(<strong>x</strong>|θ) is the joint probability function
        of the random variables <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>) and the parameter θ.
        The likelihood function can be used to relate teh prior and posterior distributions,
      </p>
      <p class='formula'>
         ζ(θ|<strong>x</strong>) ∝ f<sub>n</sub>(<strong>x</strong>|θ) ζ(θ)
      </p>
      <p>
        The constant of proportionality can be found from equating the total probability to 1.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 390)
      </p>
      <p>
        A <strong>conjugate family of prior distributions</strong> is a family of possible distributions for ζ(θ) where 
        the posterior distribution also belongs to the same family. 
        The family of gamma distributions is a conjugate family of prior distributions for Poisson distributions of
        the random variables X<sub>1</sub>, ... X<sub>n</sub> when the parameter θ is unknown.
        The family of normal distributions is itself a conjugate family of prior distributions for normal distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the mean is unknown but the variance is known.
        The family of gamma distributions is also a conjugate family of prior distributions for exponential distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the value of the parameter θ is unknown.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 395-402)
      </p>
      <p>
        An <strong>estimator</strong> δ(X<sub>1</sub>, ... X<sub>n</sub>) gives an estimate of the parameter θ using
        observed values of the data <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>). 
        A <strong>loss function</strong> L(θ, a) quantifies the effect of the difference between the estimate a of θ
        and the true value.
        A <strong>Bayes estimator</strong> δ*(<strong>x</strong>) minimizes the expected value of the loss function.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 408-409)
      </p>
      <p>
        The squared error loss function is defined as 
      </p>
      <p class='formula'>
         L(θ, a) = (θ - a)<sup>2</sup>
      </p>
      <p>
        When the squared error loss function is used the Bayes estimator is the posterior mean value of θ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 411)
      </p>
      <p>
        Another approach to estimating parameters maximizes the probability of observed data.
        A <strong>likelihood function</strong> f<sub>n</sub>(<strong>x</strong>|θ) is a joint pdf for 
        continuous random variables or joint pf
        for discrete random variables with the parameter θ considered part of the joint distribution.
        A <strong>maximum likelihood estimator</strong> maximizes the value of f<sub>n</sub>(<strong>x</strong>|θ).
        (DeGroot and Morris, <i>Probability and Statistics</i>, 418)
        This approach avoids the need to assume a probability distribution for θ. However, the drawback is that
        is may not always give a good estimate for θ and sometimes it may not exist at all.
      </p>
      <h3><a name="references"></a>References</h3>
      <h3><a name="linear"></a>Linear Regression</h3>
      <ol>
        <li>
          Amies, Alex. NTI Buddhist Text Reader GitHub Project, 2014. 
          <a href="https://github.com/alexamies/buddhist-dictionary">https://github.com/alexamies/buddhist-dictionary</a>.
        </li>
        <li>
          Bird, Steven, Ewan Klein, and Edward Loper. <i>Natural Language Processing with Python</i>. O’Reilly Media, Inc., 2009.
        </li>
        <li>
          DeGroot, Morris H., and Mark J. Schervish. <i>Probability and Statistics</i>. 4 edition. Boston: Pearson, 2011.
        </li>
        <li>
          Knell, Robert. <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and Programming in R</i>. United Kingdom: Self published, 2013.
        </li>
        <li>
          Krippendorff, Klaus H. <i>Content Analysis: An Introduction to Its Methodology</i>. Third Edition edition. SAGE Publications, Inc, 2012.
        </li>
        <li>
          The R Project for Statistical Computing (version Pumpkin Helmet). R Foundation, 2014. 
          <a href="http://www.r-project.org/">http://www.r-project.org</a>.
        </li>
      <hr/>
      <p>
        Copyright Nan Tien Institute 2013 - 2014, 
        <a href="http://www.nantien.edu.au/" title="Fo Guang Shan Nan Tien Institute">www.nantien.edu.au</a>.
      </p>
      <p>This page was last updated on November 15, 2014.</p>
     </div>
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script>
  </body>
</html>
