<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="NTI Buddhist Text Reader">
    <title>NTI Buddhist Text Reader</title>
    <link rel="shortcut icon" href="images/yan.png" type="image/jpeg" />
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="buddhistdict.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="starter-template">
      <div class="row">
        <div class="span2"><img id="logo" src="images/yan.png" alt="Logo" class="pull-left"/></div>
        <div class="span7"><h1>NTI Buddhist Text Reader</h1></div>
      </div>
    </div>
    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Home</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="corpus.html">Texts</a></li>
            <li class="active"><a href="tools.html">Tools</a></li>
            <li><a href="dict_resources.html">Resources</a></li>
            <li><a href="about.html">About</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
      <h2>Statistical Methods</h2>
      <h3>Contents</h3>
      <p>
        <ul>
          <li><a href="#probability">Probability</a></li>
          <li><a href="#tools">Software Tools</a></li>
          <li><a href="#expectation">Expectation</a></li>
          <li><a href="#special">Special Distributions</a></li>
          <li><a href="#estimation">Statistical Estimation</a></li>
          <li><a href="#references">References</a></li>
        </ul>
      </p>
      <p>
        This page gives some incomplete notes on statistical methods in analyzing Chinese text 
        to solve text analysis, historical linguistcs, and content analysis problems.
      </p>
      <h3><a name="probability"></a>Probability</h2>
      <p>
        The <strong>conditional probability</strong> of an event A given that we know an event B has already occured is written Pr(A|B).
        It can be computed as 
      </p>
      <p class='formula'>
        Pr(A|B) = Pr(A ∩ B) / Pr(B)
      </p>
      <p>
        This assumes that Pr(B) &gt; 0. (DeGroot and Morris, <i>Probability and Statistics</i>, 56)
      </p>
      <p>
        <strong>Example</strong>: Suppose that we wish to find the probability finding the character 說 at a particular position in the text. 
        In modern Chinese, the sequence 我說 is very common.
        So, if we know the previous character is 我 then the probability may be higher than the probability of finding the character 說
        alone. That is, Pr(A=說|B=我) &gt; Pr(A=說).
      </p>
      <p>
        Two events A and B are <strong>independent</strong> if the occurence of one does not affect the occurrence of the other. 
        If this is true then
      </p>
      <p class='formula'>
        Pr(A ∩ B) = Pr(A) Pr(B)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 66)
      </p>
      <p>
        A <strong>random variable</strong> is a real-valued function in a sample space S. (DeGroot and Morris, <i>Probability and Statistics</i>, 93)
      </p>
      <p>
        The <strong>probability function</strong> of a discrete random variable X is the function
      </p>
      <p class='formula'>
        f(x) = Pr(X = x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 96)
        In linguistics discrete random variables, like word frequency, are more common but continuous random
        variables may also be used, for example the word frequency of a specific word per 1,000 words of text.
        The equivalent function for a continuous random variable is called the 
        <strong>probability density function</strong> (pdf).
      </p>
      <p>
        The <strong>cummulative distribution function</strong> (cdf) of a random variable X is 
      </p>
      <p class='formula'>
        F(x) = Pr(X ≤ x) for -∞ &lt; x &lt; ∞
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 108)
      </p>
      <p>
        The <strong>quantile function</strong> F<sup>-1</sup>(p) of a random variable X is the inverse of the cdf,
        which is also the smallest value x  with F(x) ≥ p. The variable p is the probability.
        F<sup>-1</sup>(p) is the p quantile of X or 100p percentile.
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 112)
      </p>
      <p>
        The <strong>joint probability function</strong> of two random variables X and Y is 
      </p>
      <p class='formula'>
        f(x, y) = Pr(X = x and Y = y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 119)
      </p>
      <p>
        The <strong>marginal cdf</strong> of a joint probability function of two discrete random variables X and Y is 
        summed over all possible values of y. In symbols,
      </p>
      <p class='formula'>
        f<sub>1</sub>(x) = Σ<sub>All y</sub> f(x, y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 131)
      </p>
      <p>
        A <strong>stochastic process</strong> is a sequence of random variables X<sub>1</sub>, X<sub>2</sub>, ... at discrete points
        in time. A <strong>Markov chain</strong> is a stochastic process where the conditional distributions of all X<sub>n+j</sub>
        depend only on X<sub>n</sub> and not only earlier states.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 188)
      </p>
      <p>
        The <strong>transition distributions</strong> of a Markov chain are the conditional probabilities 
      </p>
      <p class='formula'>
        p<sub>ij</sub> = Pr(X<sub>n+1</sub>=j|X<sub>n</sub>=i)
      </p>
      <p>
        where the random variables X<sub>n</sub> can have k possible states.
        A <strong>transition matrix</strong> is a matrix <strong>P</strong> = [p<sub>ij</sub>] made up of the conditional probabilities of the 
        transition distributions.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 190-192)
      </p>
      <p>
        <strong>Example</strong>: A stream of words can be thought of as a Markov chain. The earlier words can influence the later
        words. In a simple statistical model, each word may only be influenced by the preceding words. 
      </p>
      <h3><a name="tools"></a>Software Tools</h3>
      <p>
        There are a number of software tools for statistical modelling and may be used for text analysis.
        Initially, R will be discussed.
      </p>
      <h4>R Project for Statistical Computing</h4>
      <p>
        The R Project for Statistical Computing (<a href="http://www.r-project.org/">http://www.r-project.org</a>),
        or simply R, is an open source project, language, and platform.
        You can freely download and install R for Linux, Mac, and Windows from links in the project web site.
        After installing it, open the command line interpreter with the command <code>R</code>.
      </p>
      <h3><a name="expectation"></a>Expectation</h2>
      <p>
        The <strong>expectation</strong> of a random variable is its <strong>mean</strong>.
        The expectation of a discrete random variable X with probability function f is defined as 
      </p>
      <p class='formula'>
        E(X) = Σ<sub>All x</sub> x f(x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 208)
      </p>
      <p>
        The <strong>variance</strong> of a discrete random variable X with mean μ is defined as
      </p>
      <p class='formula'>
        Var(X) = E[(X - μ)<sup>2</sup>]
      </p>
      <p>
        The <strong>standard deviation</strong> is the square root of the variance.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 226)
      </p>
      <p>
        The <strong>covariance</strong> of random variables X and Y with means μ<sub>x</sub> and μ<sub>y</sub> 
        is defined as
      </p>
      <p class='formula'>
        Cov(X, Y) = E[(X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)]
      </p>
      <p>
        assuming that the expectation exists.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 248)
      </p>
      <p>
        The <strong>correlation</strong> of random variables X and Y with variances σ<sub>x</sub><sup>2</sup> and 
        σ<sub>y</sub><sup>2</sup> is defined as
      </p>
      <p class='formula'>
        ρ(X, Y) = Cov(X, Y) / [σ<sub>x</sub>σ<sub>y</sub>]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 250)
      </p>
      <h3><a name="special"></a>Special Distributions</h2>
      <p>
        The <strong>Bernoulli distribution</strong> for random variable X, which can only take the values 0 and 1,
        with parameter p (0 ≤ p ≤ 1) has the probabilities
      </p>
      <p class='formula'>
        Pr(X = 1) = p and Pr(X = 0) = 1 - p
      </p>
      <p>
        An sequence random variables with the Bernoulli distribution are called <strong>Bernoulli trials</strong>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 276)
      </p>
      <p>
        The <strong>Binomial distribution</strong> with integer parameter n and continuous parameter p (0 ≤ p ≤ 1)
        is defined as 
      </p>
      <p class='formula'>
         f(x|n,p) = (n ¦ x) p<sup>x</sup>(1 - p)<sup>n-x</sup> for x = 0, 1, 2, ... and 0 otherwise
      </p>
      <p>
        where (n ¦ x) is the binomial coefficient n!/[x!(n - x)].
        The mean and variance are
      </p>
      <p class='formula'>
         E(X) = np
      </p>
      <p class='formula'>
         Var(X) = np(1 - p)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 277)
      </p>
      <p>
        The <strong>Poisson distribution</strong> for random variable X with mean λ is defined as
      </p>
      <p class='formula'>
         f(x|λ) = e<sup>-λ</sup> λ<sup>x</sup>/x! for x = 0, 1, 2, ... and 0 otherwise.
      </p>
      <p>
        The variance of the Poisson distribution is also λ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 288-290)
      </p>
      <p>
        The <strong>normal distribution</strong> for the continuous random variable X with mean μ and 
        standard deviation σ is defined as
      </p>
      <p class='formula'>
         f(x|μ, σ) = [1/σ√(2π)] exp[-0.5((x - μ)/σ)<sup>2</sup>] for -∞ &lt; x &lt; ∞
      </p>
      <p>
        The normal distribution is a good approximation for variables in many random processes.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 303)
        The <strong>Central Limit Theorem</strong> states that the distribution of a sum of 
        random variables Σ<sub>i=1</sub><sup>n</sup> X<sub>i</sub>  with any distribution will be
        approximately the normal distribition with mean nμ and variance nσ<sup>2</sup>, as
        n becomes large.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 361)
      </p>
      <p>
        The <strong>gamma distribution</strong> for the continuous random variable X with parameters 
        α and β is 
      </p>
      <p class='formula'>
         f(x|α, β) = [β<sup>α</sup> / Γ(α)] x<sup>α-1</sup> e<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        where Γ(α) is the gamma function. 
        The mean of the gamma distribution is α/β and the variance is α/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 319-320)
      </p>
      <p>
        The <strong>exponential distribution</strong> for the continuous random variable X with parameter β is
      </p>
      <p class='formula'>
         f(x|β) = βe<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        The exponential distribution is a special case of the gamma distribution with α = 1.
        The mean of the exponential distribution is 1/β and the variance is 1/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 321)
      </p>
      <p>
        The <strong>beta distribution</strong> for the continuous random variable X with parameters α and β is
      </p>
      <p class='formula'>
         f(x|α, β) = [Γ<sup>β+α</sup> / (Γ(α) Γ(β))] x<sup>α-1</sup> (1 - x)<sup>β-1</sup> for 0 &lt; x &lt; 1 or 0 otherwise
      </p>
      <p>
        The mean of the beta distribution is 
      </p>
      <p class='formula'>
        E(X) = α/(α + β)
      </p>
      <p>
        The variance of the beta distribution is 
      </p>
      <p class='formula'>
        Var(X) = αβ/[(α + β)<sup>2</sup>(α + β + 1)]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 328-329)
      </p>
      <p>
        The <strong>multinomial distribution</strong> for the discrete random vector 
        <strong>X</strong> = (X<sub>1</sub>, X<sub>2</sub>, ... X<sub>k</sub>) having probabilities
        <strong>p</strong> = (p<sub>1</sub>, p<sub>2</sub>, ... p<sub>k</sub>) 
        with n items selected is defined as
      </p>
      <p class='formula'>
         f(<strong>X</strong>|n, <strong>p</strong>) = [n!/(x<sub>1</sub> x<sub>2</sub> ... x<sub>k</sub>)]
         p<sub>1</sub><sup>x<sub>1</sub></sup> p<sub>2</sub><sup>x<sub>2</sub></sup> ... p<sub>k</sub><sup>x<sub>k</sub></sup>
      </p>
      <p>
        if x<sub>1</sub> + x<sub>2</sub> + ... x<sub>k</sub> = n or 0 otherwise.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 334)
        The multinomial distribution is appropriate for distributions into sets that are not necessarily numbers,
        for example, the frequencies of words of different part of speech values.
      </p>
      <h3><a name="estimation"></a>Statistical Estimation</h3>
      <p>
        A <strong>statistical model</strong> is a collection of random variables, identification of probability
        distributions for the variables, and the set of parameters that the distributions require values for.
        <strong>Statistical inference</strong> is a probabilistic statement about a statistical model.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 377-378)
        For example, the approximate date of a document may be inferred from the vocabulary in it.
        (Krippendorff, <i>Content Analysis</i>, 42)
        In a Buddhist text mention of copying sutras or description of devotional practices may help provide information for the date
        for the text. The data may be recorded as 0 (not present) and 1 (present) or as a word frequency.
      </p>
      <p>
        A <strong>statistic</strong> is a function of a set of random variables. For example, mean, median, 
        and variance are statistics.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 382)
      </p>
      <p>
        Parameters in probability distributions are usually unknown and needed to be estimated with statistical methods.
        So the parameters themselves can be considered simply as unknown constants or to have probability distributions
        themselves.
        The <strong>prior distribution</strong> of a parameter θ is the probability distribution ζ(θ) assumed before
        experimental observations are applied to estimate its value.
        The <strong>posterior distribution</strong> ζ(θ|x<sub>1</sub>, ... x<sub>n</sub>) 
        is the conditional distribution after the random variables X<sub>1</sub>, ... X<sub>n</sub> have been observed.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 385-387)
        The <strong>likelihood function</strong> f<sub>n</sub>(<strong>x</strong>|θ) is the joint probability function
        of the random variables <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>) and the parameter θ.
        The likelihood function can be used to relate teh prior and posterior distributions,
      </p>
      <p class='formula'>
         ζ(θ|<strong>x</strong>) ∝ f<sub>n</sub>(<strong>x</strong>|θ) ζ(θ)
      </p>
      <p>
        The constant of proportionality can be found from equating the total probability to 1.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 390)
      </p>
      <p>
        A <strong>conjugate family of prior distributions</strong> is a family of possible distributions for ζ(θ) where 
        the posterior distribution also belongs to the same family. 
        The family of gamma distributions is a conjugate family of prior distributions for Poisson distributions of
        the random variables X<sub>1</sub>, ... X<sub>n</sub> when the parameter θ is unknown.
        The family of normal distributions is itself a conjugate family of prior distributions for normal distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the mean is unknown but the variance is known.
        The family of gamma distributions is also a conjugate family of prior distributions for exponential distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the value of the parameter θ is unknown.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 395-402)
      </p>
      <p>
        An <strong>estimator</strong> δ(X<sub>1</sub>, ... X<sub>n</sub>) gives an estimate of the parameter θ using
        observed values of the data <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>). 
        A <strong>loss function</strong> L(θ, a) quantifies the effect of the difference between the estimate a of θ
        and the true value.
        A <strong>Bayes estimator</strong> δ*(<strong>x</strong>) minimizes the expected value of the loss function.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 408-409)
      </p>
      <p>
        The squared error loss function is defined as 
      </p>
      <p class='formula'>
         L(θ, a) = (θ - a)<sup>2</sup>
      </p>
      <p>
        When the squared error loss function is used the Bayes estimator is the posterior mean value of θ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 411)
      </p>
      <h3><a name="references"></a>References</h3>
      <ol>
        <li>
          DeGroot, Morris H., and Mark J. Schervish. <i>Probability and Statistics</i>. 4 edition. Boston: Pearson, 2011.
        </li>
        <li>
          Knell, Robert. <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and Programming in R</i>. United Kingdom: Self published, 2013.
        </li>
        <li>
          Krippendorff, Klaus H. <i>Content Analysis: An Introduction to Its Methodology</i>. Third Edition edition. SAGE Publications, Inc, 2012.
        </li>
        <li>
          The R Project for Statistical Computing (version Pumpkin Helmet). R Foundation, 2014. 
          <a href="http://www.r-project.org/">http://www.r-project.org</a>.
        </li>
      <hr/>
      <p>
        Copyright Nan Tien Institute 2013 - 2014, 
        <a href="http://www.nantien.edu.au/" title="Fo Guang Shan Nan Tien Institute">www.nantien.edu.au</a>.
      </p>
      <p>This page was last updated on November 8, 2014.</p>
     </div>
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script>
  </body>
</<html>
