<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="NTI Buddhist Text Reader">
    <title>NTI Buddhist Text Reader</title>
    <link rel="shortcut icon" href="images/yan.png" type="image/jpeg" />
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="buddhistdict.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="starter-template">
      <div class="row">
        <div class="span2"><img id="logo" src="images/yan.png" alt="Logo" class="pull-left"/></div>
        <div class="span7"><h1>NTI Buddhist Text Reader</h1></div>
      </div>
    </div>
    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Home</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="corpus.html">Texts</a></li>
            <li class="active"><a href="tools.html">Tools</a></li>
            <li><a href="dict_resources.html">Resources</a></li>
            <li><a href="about.html">About</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
      <h2>Statistical Background</h2>
      <h3>Contents</h3>
      <p>
        <ul>
          <li><a href="#data">Corpus Data</a></li>
          <li><a href="#tools">Software Tools</a></li>
          <li><a href="#probability">Probability</a></li>
          <li><a href="#expectation">Expectation</a></li>
          <li><a href="#special">Special Distributions</a></li>
          <li><a href="#estimation">Statistical Estimation</a></li>
          <li><a href="#sampling">Sampling</a></li>
          <li>
            <a href="#hypothesis">Hypothesis Testing</a>
            <ul>
              <li><a href="#hypothesisconcepts">Concepts</a></li>
              <li><a href="#twomeans">Comparing means from two distributions</a></li>
              <li><a href="#fdist">F Distribution</a></li>
            </ul>
          </li>
          <li>
            <a href="#categorical">Categorical Data</a>
            <ul>
              <li><a href="#goodness">χ<sup>2</sup> goodness of fit test</a></li>
              <li><a href="#chisqcomposite">χ<sup>2</sup> Test for a Composite Hypothesis</a></li>
              <li><a href="#contingency">Contingency Tables</a></li>
            </ul>
          </li>
          <li><a href="#linear">Linear Regression</a></li>
          <li><a href="#references">References</a></li>
        </ul>
      </p>
      <p>
        This page gives some incomplete notes on statistical background in analyzing Chinese text 
        and analyzing quality of corpus data.
      </p>
      <h3><a name="data"></a>Corpus Data</h3>
      <p>
        A <strong>corpus</strong> is a collection of texts, which may be used by linguists for studying the 
        characteristics of language usage. (Bird, <i>Natural Language Processing</i>, 39)
        The NTI Buddhist Text Reader contains a corpus of Buddhist and classical Chinese texts.
        A small subset of the corpus is tagged part-of-speech taggs and gloss distinguishing word sense.
        The organization of the corpus is described on the page
        <a href="annotation.html">Text Management, Annotation, and Gloss</a> on this web site.
        The tagging scheme is described on the page <a href="pos_tags.html">Part-of-Speech Tag Definitions</a>.
        The raw data, including word frequencies, can be found at the 
        <a href="https://github.com/alexamies/buddhist-dictionary">NTI Buddhist Text Reader GitHub Project</a> in the 
        data/dictionary folder. They are tab delimited text files.
      </p>
      <p>
        Word frequencies from the tagged subset of the corpus can found in the files
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/dictionary/unigram.txt">unigram.txt</a>
        and 
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/dictionary/bigram.txt">bigram.txt</a>
        These are a tab delimited UTF-8 file with no header row.
        The unigram.txt file lists single word frequencies in the tagged corpus. 
        The structure of the unigram.txt file is:
      </p>
      <p class='formula'>
        <pre>
          pos_tagged_text: The element text with POS tag and gloss in pinyin and English
          element_text:    The element text in traditional Chinese
          word_id:         Matching id in the word table (positive integer)
          frequency:       The frequency of occurence of the word sense (positive integer)
        </pre>
      </p>
      <p>
        The bigram.txt file lists frequencies of two-word combinations.
        The structure of the bigram.txt file is:
      </p>
      <p class='formula'>
        <pre>
          pos_tagged_text: The element text with POS tag and gloss in pinyin and English
          previous_text:   The element text in traditional Chinese
          element_text:    The element text in traditional Chinese
          word_id:         Matching id in the word table (positive integer)
          frequency:       The frequency of occurence of the word sense (positive integer)
        </pre>
      </p>
      <p>
        A tab delimited plain text list of summary statistcs for the corpus texts is included in the file 
        <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/data/stats/corpus_stats.txt">corpus_stats.txt</a>. 
        The structure of the file is 
      </p>
      <p class='formula'>
        <pre>
          source_name: the title of the text
          word_count: the number of words in the text
          character_count: the number of characters in the text
          unique_words: the number of unique words in the text
        </pre>
      </p>
      <h3><a name="tools"></a>Software Tools</h3>
      <p>
        There are a number of software tools for statistical modelling and may be used for text analysis.
        Initially, R will be discussed.
      </p>
      <h4>R Project for Statistical Computing</h4>
      <p>
        The R Project for Statistical Computing (<a href="http://www.r-project.org/">http://www.r-project.org</a>),
        or simply R, is an open source project, language, and platform.
        You can freely download and install R for Linux, Mac, and Windows from links in the project web site.
        The R project web site has links to introductory materials. 
        I referred to the books by Knell 
        (Knell,  <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and 
        Programming in R</i>) and Yau
        (Yau, <i>R Tutorial with Bayesian Statistics Using OpenBUGS</i>)
        in preparing the commands and scripts presented here.
      </p>
      <p>
        R is a generic statistics platform. It is very handy for working through statistics problems in textbooks
        and trying stuff out rather than using a piece of paper and calculator or a spreadsheet.
        In addition, there are a number of packages for R specifically for text analysis. 
        These can be found in the web page 
        <a href="http://cran.r-project.org/web/packages/available_packages_by_name.html">Available CRAN Packages By Name</a>.
        Some examples are: 
        <a href="http://cran.r-project.org/web/packages/koRpus/index.html">koRpus: An R Package for Text Analysis</a>,
        <a href="http://cran.r-project.org/web/packages/tau/index.html">tau: Text Analysis Utilities</a>,
        <a href="http://cran.r-project.org/web/packages/textcat/index.html">textcat: N-Gram Based Text Categorization</a>,
        and <a href="http://cran.r-project.org/web/packages/tm/index.html">tm: Text Mining Package</a>.
      </p>
      <p>
        After installing R, open the command line interpreter with the command <code>R</code>.
        The unigram.txt file can be read in to a data frame using the <code>read.table()</code> function, as shown below.
        Change directories to the directory containing the file first.
      </p>
      <p class='formula'>
        <pre>
          $ R
          . . .
          &gt; names &lt;- c("pos.tagged.text", "element.text", "word.id", "frequency")
          &gt; unigram &lt;- read.table("unigram.txt", header=FALSE, sep="\t", quote="\"", col.names=names, numerals ="allow.loss")
          &gt; head(unigram)
                       pos.tagged.text element.text word.id frequency
          1 須菩提/NR[xūpútí | Subhuti]        須菩提    6645       137
          2           是/PN[shì | this]           是   17908       136
          3             不/AD[bù | not]           不     502       130
          4          佛/NR[Fó | Buddha]           佛    3618       130
          5               於/P[yú | in]           於    1710        93
          6  如來/NR[Rúlái | Tathagata]          如來    6686        89
        </pre>
      </p>
      <p>
        The <code>$</code> prompt is shown before a shell command and the <code>&gt;</code> prompt is shown 
        before an R command.
        The <code>c()</code> function contatenates the arguments into a vector, which is assigned to the 
        variable <code>names</code> using the assignment operator <code>&lt;-</code>.
        The variable <code>names</code> is used for the column names later.
        The unigram.txt file into the <code>unigram</code> data frame with the <code>read.table()</code> function.
        The <code>head</code> function prints out the first few lines of the data frame.
        The UTF-8 encoded Chinese characters are read in by R correctly.
        The NTI Reader text files are formatted to be loaded into MySQL and there are a few differences with the 
        basic form of the <code>read.table()</code> function.
        The format for <code>NULL</code> values in MySQL is <code>\N</code> but R expects <code>NA</code>.
        So, you will need to be careful if you depend on accurate representation of <code>NULL</code> values.
        In addition, the NTI Reader files do not have variable names in the first row.
      </p>
      <p>
        The bigram.txt file can be read in to a data frame in a similar way, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; binames &lt;- c("pos.tagged.text", "previous.text", "element.text", "word.id", "frequency")
          &gt; bigram &lt;- read.table("bigram.txt", header=FALSE, sep="\t", quote="\"", col.names=binames, numerals ="allow.loss")
          &gt; head(bigram)
                 pos.tagged.text previous.text element.text word.id frequency
          1   故/NN[gù | purpose]            以           故    7115        38
          2    以/P[yǐ | because]            何           以   30648        38
          3 云何/DT[yúnhé | what]            意         云何   29319        32
          4      意/NN[yì | idea]            於           意    1730        30
          5    佛/NR[Fó | Buddha]            諸           佛    3618        26
          6  說/VV[shuō | speaks]          如來           說     412        23

        </pre>
      </p>
      <p>
        You can make a plot of data in R using the <code>plot()</code> function, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; x &lt;- seq(1, length(unigram$frequency))
          &gt; plot(x, unigram$frequency, xlab="", ylab="Frequency", pch=20, col="blue")
        </pre>
      </p>
      <p>
        The <code>seq()</code> function genrates a sequence of integers so that the frequency
        of each word will be plotted from most frequent at the left to least frequent at the right. 
        The <code>type="n"</code> parameter of the <code>plot()</code> function defers plotting of the points 
        to the next line where the <code>points()</code> function plots the points with symbol 20 and blue color.
        There are so many words that we cannot show the text for each one but it 
        is informative to see the general shape of the word frequency distribution.
        The image generated is shown in Figure 1.
      </p>
      <p class='picture'>
        <img src="images/unigram_frequency.png"/><br/>
        Figure 1: Word Frequency in the unigram Data Frame
      </p>
      <p>
        To generate an image file for that plot and the others in this page, pull the project from GitHub, 
        change to the top level directory in the project and type the command 
      </p>
      <p class='formula'>
        <pre>
          $ Rscript r/generate_images.R
        </pre>
      </p>
      <p>
        This is a useful diagram because it shows the breadth and depth of the corpus at a glance. The corpus
        only has about 1,400 unique words and only has a substantial frequency for less than 200 of these words.
      </p>
      <p>
        You will need to install the R <code>showtext</code> package to properly display Chinese text on R generated graphics.
        Use the commands below to install <code>showtext</code>.
      </p>
      <p class='formula'>
        <pre>
          &gt; install.packages("showtext")
          &gt; library(showtext)
        </pre>
      <p>
        You can make a histogram plot of the frequency data using the <code>barplot()</code> function, as shown below.
      </p>
      <p class='formula'>
        <pre>
          &gt; freq &lt;- as.vector(as.matrix(unigram[4])[1:8, 1])
          &gt; freq.labels &lt;- as.vector(as.matrix(unigram[2])[1:8, 1])
          &gt; showtext.begin()
          &gt; barplot(freq, names.arg=freq.labels, ylab="Frequency", col="steelblue")
          &gt; showtext.end()
        </pre>
      </p>
      <p>
        A subset of fourth column of the unigram data.frame is read into the variable <code>freq</code>, after being converted to a vector.
        The labels are the Chinese text for each word taken from column 2.
        Rather than generate the graph in a window, the code above writes it to a png file.
        The <code>showtext.begin()</code> needs to be called before generating the graph.
        When <code>dev.off()</code> is called the file will be written.
        The chart generated is shown in Figure 2.
      </p>
      <p class='picture'>
        <img src="images/ungram_barplot.png"/><br/>
        Figure 2: Word Frequency Bar Plot for a Subset of the Data
      </p>
      <h3><a name="probability"></a>Probability</h2>
      <p>
        <strong>Example</strong>: Different Chinese characters have acquired multiple meanings over history.
        This makes choosing the appropriate word meaning require understanding of the context.
        One of the goals of the NTI Reader is to help the user decide which is the most appropriate word sense.
        Suppose that we wish to find the probability of choosing the right meaning of the character 法 (fǎ).
        法 (fǎ) most often means the noun "law" or the noun "method" in modern Chinese. 
        However, in Buddhist texts written in literary Chinese, 法 might mean the proper noun "Dharma" 
        (the teachings of the Buddha), teachings in general, or the noun "dharma" (phenomenon).
        The word frequencies for each sense can be found from the unigram table using the following
        R commands.
      </p>
      <p class='formula'>
        <pre>
          &gt; fa &lt;- subset(unigram, element.text=="法")
          &gt; fa
                          pos.tagged.text element.text word.id frequency
          10          法/NN[fǎ | a dhárma]           法   17994        74
          1253          法/NR[Fǎ | Dhárma]           法    3509         1
          1376 法/NN[fǎ | a mental object]           法   32204         1
          1397          法/NN[fǎ | method]           法    3506         1
          &gt; PrA = fa$frequency[1] / sum(fa$frequency)
          &gt; PrA
          [1] 0.961039
        </pre>
      </p>
      <p>
        The <code>subset()</code> function selects a subset of the unigram table with the value of element_text equal to "法".
        The total number of occurences of 法 in the tagged corpus is <code>sum(fa$frequency) = 74 + 1 + 1 + 1 = 77</code>.
        So the probability of any one occurence of 法 being the noun (NN) meaning "a dhárma" (a teaching) is 
        <code>74/77 = 0.961</code>. 
        If we had no other information about the context of a occurence of 法, then we would guess that it would mean
        a dhárma. ▢
      </p>
      <p>
        The <strong>conditional probability</strong> of an event A given that we know an event B has already occured is written Pr(A|B).
        It can be computed as 
      </p>
      <p class='formula'>
        Pr(A|B) = Pr(A ∩ B) / Pr(B)
      </p>
      <p>
        This assumes that Pr(B) &gt; 0. (DeGroot and Morris, <i>Probability and Statistics</i>, 56)
      </p>
      <p>
        <strong>Example</strong>: If we know the word before 法, it may give us a better chance at picking the correct
        word sense. This is an example of conditional probability
        We can use the bigram table to compute the conditinal probability of the word sense of 法.
        Suppose the word before is 說 shuō "to say" in modern Chinese but more commonly "to teach" in 
        literary Chinese.
        That would make the phrase be 說法 "to teach a dhárma."
        Let A = the proper meaning of 法 in this instance is 法/NN[fǎ | a dhárma].
        Let B = the word before 法 is 說.
        The conditional probability can be computed using the R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; subset(subset(bigram, element.text=="法"), previous.text=="說")
                 pos.tagged.text previous.text element.text word.id frequency
          34 法/NN[fǎ | a dhárma]            說           法   17994         9
        </pre>
      </p>
      <p class='formula'>
        Pr(A|B) = 9 / 9 = 1.0
      </p>
      <p>
        That is, whenever the word before 法 is 說 then the proper word sense is always 法/NN[fǎ | a dhárma].
        The interpretation of this is that the previous word is a very good predictor of word sense,
        in this case. ▢
      </p>
      <p>
        Two events A and B are <strong>independent</strong> if the occurence of one does not affect the occurrence of the other. 
        If this is true then
      </p>
      <p class='formula'>
        Pr(A ∩ B) = Pr(A) Pr(B)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 66)
      </p>
      <p>
        A <strong>random variable</strong> is a real-valued function in a sample space S. (DeGroot and Morris, <i>Probability and Statistics</i>, 93)
      </p>
      <p>
        The <strong>probability function</strong> of a discrete random variable X is the function
      </p>
      <p class='formula'>
        ƒ(x) = Pr(X = x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 96)
        In linguistics discrete random variables, like word frequency, are more common but continuous random
        variables may also be used, for example the word frequency of a specific word per 1,000 words of text.
        The equivalent function for a continuous random variable is called the 
        <strong>probability density function</strong> (pdf).
      </p>
      <p>
        The <strong>cummulative distribution function</strong> (cdf) of a random variable X is 
      </p>
      <p class='formula'>
        F(x) = Pr(X ≤ x) for -∞ &lt; x &lt; ∞
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 108)
      </p>
      <p>
        The <strong>quantile function</strong> F<sup>-1</sup>(p) of a random variable X is the inverse of the cdf,
        which is also the smallest value x  with F(x) ≥ p. The variable p is the probability.
        F<sup>-1</sup>(p) is the p quantile of X or 100p percentile.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 112)
        A <strong>quartile</strong> is found by sorting the data and then dividing it into four equal groups.
        The <strong>interquartile range</strong> is the middle two quartiles or, in other words, the range 
        between the 25 and 75th percentiles.
        Quartile and range information can be found using the R function <code>summary()</code>.
        The quantile can be found using the R function <code>qt()</code>.
        The interquartile range can be found using the R function <code>IQR()</code>.
      </p>
      <p>
        <strong>Example</strong>: 
        Summary data for the word frequency data can be found as follows.
      </p>
      <p class='formula'>
        <pre>
          &gt; summary(unigram$frequency)
          Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
          1.000   1.000   1.000   4.708   3.000 137.000 
          &gt; length(unigram$frequency)
          [1] 1432
        </pre>
      </p>
      <p>
        The <code>length()</code> function gives the number of items in the data set, which shows that there
        are only 1,432 unique words in the tagged corpus. ▢
      </p>
      <p>
        The <strong>joint probability function</strong> of two random variables X and Y is 
      </p>
      <p class='formula'>
        ƒ(x, y) = Pr(X = x and Y = y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 119)
      </p>
      <p>
        The <strong>marginal cdf</strong> of a joint probability function of two discrete random variables X and Y is 
        summed over all possible values of y. In symbols,
      </p>
      <p class='formula'>
        ƒ<sub>1</sub>(x) = ∑<sub>All y</sub> ƒ(x, y)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 131)
      </p>
      <p>
        A <strong>stochastic process</strong> is a sequence of random variables X<sub>1</sub>, X<sub>2</sub>, ... at discrete points
        in time. A <strong>Markov chain</strong> is a stochastic process where the conditional distributions of all X<sub>n+j</sub>
        depend only on X<sub>n</sub> and not only earlier states.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 188)
      </p>
      <p>
        The <strong>transition distributions</strong> of a Markov chain are the conditional probabilities 
      </p>
      <p class='formula'>
        p<sub>ij</sub> = Pr(X<sub>n+1</sub>=j|X<sub>n</sub>=i)
      </p>
      <p>
        where the random variables X<sub>n</sub> can have k possible states.
        A <strong>transition matrix</strong> is a matrix <strong>P</strong> = [p<sub>ij</sub>] made up of the conditional probabilities of the 
        transition distributions.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 190-192)
      </p>
      <p>
        <strong>Example</strong>: A stream of words can be thought of as a Markov chain. The earlier words can influence the later
        words. In a simple statistical model, each word may only be influenced by the preceding words. 
      </p>
      <h3><a name="expectation"></a>Expectation</h2>
      <p>
        The <strong>expectation</strong> or of a random variable is its <strong>mean</strong>.
        The expectation of a discrete random variable X with probability function f is defined as 
      </p>
      <p class='formula'>
        E(X) = Σ<sub>All x</sub> x ƒ(x)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 208)
        The <strong>median</strong> is another measure of central tendency, which separates the lower half from
        the upper half of a set of numbers. It can be more useful than the mean
        when dealing with small integers or highly skewed data.
        The mean of a vector of numbers can be found with the R function <code>mean()</code> and the median 
        can be found with the function <code>median()</code>.
      </p>
      <p>
        <strong>Example</strong>: 
        The values of mean and median for word frequency in the NTI Reader tagged corpus can be found with the 
        R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; mean(unigram$frequency)
          [1] 4.708101
          &gt; median(unigram$frequency)
          [1] 1
        </pre>
      </p>
      <p>
        This can give some idea of the adequacy of the size of the tagged corpus.
        A mean frequency of about 4.7 word occurrences and a median of 1 in the tagged corpus makes the tagged corpus
        seem kind of small. We need to do more analysis to understand what might be really sufficient. ▢
      </p>
      <p>
        The <strong>variance</strong> of a random variable X with mean μ is defined as
      </p>
      <p class='formula'>
        Var(X) = E[(X - μ)<sup>2</sup>] 
      </p>
      <p>
        The <strong>variance</strong> of a discrete random variable X with mean μ can be computed with the sum
      </p>
      <p class='formula'>
        Var(X) = (1/n) ∑<sub>all i</sub>(X<sub>i</sub> - μ)<sup>2</sup>
      </p>
      <p>
        The <strong>standard deviation</strong> is the square root of the variance.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 226)
      </p>
      <p>
        <strong>Example</strong>: 
        The variance and standard deviation of word frequency in the NTI Reader tagged corpus can be found with the 
        R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; x &gt;- unigram$frequency
          &gt; v &lt;- sum((x-mean(x))^2)/(length(x)) 
          &gt; v
          [1] 140.0195
          &gt; sqrt(v)
          [1] 11.83299
        </pre>
      </p>
      <p>
        The variance is about 140.0 and the standard deviation about 11.8. ▢
      </p>
      <p>
        The <strong>covariance</strong> of random variables X and Y with means μ<sub>x</sub> and μ<sub>y</sub> 
        is defined as
      </p>
      <p class='formula'>
        Cov(X, Y) = E[(X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)]
      </p>
      <p>
        assuming that the expectation exists.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 248)
        The <strong>sample covariance</strong> corrects for sampling from a larger population. 
        It is given by the formula:
      </p>
      <p class='formula'>
        Cov<sub>x, y</sub> = ∑(x - x̄)(y - ȳ)/(n - 1)
      </p>
      <p>
        where x̄ and ȳ are the sample means and n - 1 is the degrees of freedom.
        (Knell, <i>Introductory R</i>, 211)
        The sample covariance can be computed using the R function <code>cov()</code>.
      </p>
      <p>
        The <strong>correlation</strong> of random variables X and Y with variances σ<sub>x</sub><sup>2</sup> and 
        σ<sub>y</sub><sup>2</sup> is defined as
      </p>
      <p class='formula'>
        ρ(X, Y) = Cov(X, Y) / [σ<sub>x</sub>σ<sub>y</sub>]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 250)
        The correlation for a sample can be computed from the formula
      </p>
      <p class='formula'>
        r = Σ(x - x̄)(y - ȳ)/[(n - 1)(s<sub>x</sub> s<sub>y</sub>)]
      </p>
      <p>
        where s<sub>x</sub> and s<sub>y</sub> are the sample standard deviations.
        (Knell, <i>Introductory R</i>, 212)
        The sample correlation can be computed using the R function <code>cor()</code>.
      </p>
     <p>
        <strong>Example</strong>: Let's find the correlation between text size
        and the number of unique words in different texts in the corpus.
        We can load the file corpus_stats.txt, containing the summary statistics of the corpus texts, 
        print out the first row of data, print a summary of the distribution of unique words,
        find the correlation, and draw a plot with these R commands.
      </p>
      <p class='formula'>
        <pre>
          &gt; corpusstats &lt;- read.table("../stats/corpus_stats.txt", sep="\t", quote="\"", numerals ="allow.loss", header=TRUE)
          &gt; corpusstats[1,]
                             source.name word.count character.count unique.words
          1 Diamond Sūtra 金剛般若波羅蜜經       3407            5307          534
          &gt; summary(corpusstats$unique.words)
          Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
          15.0   122.2   580.5   962.2  1661.0  5145.0 
          &gt; cor(corpusstats$word.count, corpusstats$unique.words)
          [1] 0.7266394
          &gt; plot(corpusstats$word.count, corpusstats$unique.words, xlab="Word Count", ylab="Unique Words", pch=17, col="blue")
        </pre>
      </p>
      <p>
        This particular file does have column headers in the first row that are automatically used by R 
        as column names in the data frame.
        The image generated is shown in Figure 3.
      </p>
      <p class='picture'>
        <img src="images/unique_words.png"/><br/>
        Figure 3: Variation of Unique Words with Text Size
      </p>
      <p>
        From the correlation value there r = 0.727 there is clearly a correlation between text length and number of 
        unique words. However, from Figure 3 we can see that it is consistent but not linear. 
        The number of unique words eventually
        begins to flatten out with really long texts. Since the set of words in a text may include proper nouns, 
        such as person and place names, the number of unique words is practically unlimited.
        It is useful to complate the summary of unique words with the unigram data for the tagged corpus.
        The third quartile value is 1661, which is greater than the total number of unique words in the 
        tagged corpus. So, there are at least 25% of the texts in the corpus that have more unique words than 
        we have word frequency data for. Clearly, we are short of coverage on word frequency.
      </p>
      <h3><a name="special"></a>Special Distributions</h2>
      <p>
        The <strong>Bernoulli distribution</strong> for random variable X, which can only take the values 0 and 1,
        with parameter p (0 ≤ p ≤ 1) has the probabilities
      </p>
      <p class='formula'>
        Pr(X = 1) = p and Pr(X = 0) = 1 - p
      </p>
      <p>
        An sequence random variables with the Bernoulli distribution are called <strong>Bernoulli trials</strong>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 276)
      </p>
      <p>
        The <strong>Binomial distribution</strong> with integer parameter n and continuous parameter p (0 ≤ p ≤ 1)
        is defined as 
      </p>
      <p class='formula'>
         ƒ(x|n,p) = (n ¦ x) p<sup>x</sup>(1 - p)<sup>n-x</sup> for x = 0, 1, 2, ... and 0 otherwise
      </p>
      <p>
        where (n ¦ x) is the binomial coefficient n!/[x!(n - x)].
        The mean and variance are
      </p>
      <p class='formula'>
         E(X) = np
      </p>
      <p class='formula'>
         Var(X) = np(1 - p)
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 277)
        The R function to compute the binomial probability is <code>dbinom()</code>.
        The commands to plot a binomial distribution with parameters n = 20 and p = 0.5 are shown below.
      </p>
      <p class='formula'>
        <pre>
         &gt; x &lt;- seq(1, 20)
         &gt; plot(x, dbinom(x, 20, 0.5), xlab="x", ylab="Frequency", pch=17, col="blue")
        </pre>
      </p>
      <p>
        The <code>plot()</code> function takes a sequence of integers <code>x</code> from 1 to 20
        and plots the binomial probability for each value.
        The graph generated is shown in Figure 4.
      </p>
      <p class='picture'>
        <img src="images/binomial05.png"/><br/>
        Figure 4: Binomial Distribution with Parameters n = 20 and p = 0.5
      </p>
      <p>
        The <strong>Poisson distribution</strong> for random variable X with mean λ is defined as
      </p>
      <p class='formula'>
         ƒ(x|λ) = e<sup>-λ</sup> λ<sup>x</sup>/x! for x = 0, 1, 2, ... and 0 otherwise.
      </p>
      <p>
        The variance of the Poisson distribution is also λ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 288-290)
        The <code>dpois()</code> R function can be used to compute values of the Poisson distribution.
        This is demonstrated with the R commands below, which generate a graph of the 
        Poisson distribution with λ = 3.0.
      </p>
      <p class='formula'>
        <pre>
         &gt; x &lt;- seq(1, 10)
         &gt; plot(x, dpois(x, 3.0), xlab="x", ylab="Frequency", pch=17, col="blue")
        </pre>
      </p>
      <p>
        The graph generated is shown in Figure 5.
      </p>
      <p class='picture'>
        <img src="images/poisson_lambda3.png"/><br/>
        Figure 5: Poisson Distribution with λ = 3.0
      </p>
      <p>
        The <strong>normal distribution</strong> for the continuous random variable X with mean μ and 
        standard deviation σ is defined as
      </p>
      <p class='formula'>
         ƒ(x|μ, σ) = [1/σ√(2π)] exp[-0.5((x - μ)/σ)<sup>2</sup>] for -∞ &lt; x &lt; ∞
      </p>
      <p>
        The <strong>standard normal distribution</strong> has mean μ = 0 and standard deviation σ = 1.
        It is given by the equation
      </p>
      <p class='formula'>
         ƒ(z|μ, σ) = [1/√(2π)] exp[-0.5((z)/σ)<sup>2</sup>] for -∞ &lt; z &lt; ∞
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 307)
        The standard normal distribution can be plotted using the <code>curve()</code> function in R, as shown below.
      <p class='formula'>
        <pre>
          &gt; curve(dnorm(x), -3, 3, xlab="z", ylab="Probability Density", col="blue")
        </pre>
      </p>
      <p>
        The graph generated is shown in Figure 6.
      </p>
      <p class='picture'>
        <img src="images/standard_normal.png"/><br/>
        Figure 6: Standard Normal Distribution
      </p>
      <p>
        A random variable X with mean μ and standard deviation σ can be transformed with to 
        the standard normal distribution with the equation
      </p>
      <p class='formula'>
         z = (x - μ)/σ
      </p>
      <p>
        The cummulative distribution F(x) of a normal distrubution can be computed in R with the function 
        <code>pnorm(q, mean = 0, sd = 1</code>.
        The quantile function F<sup>-1</sup>(x) can be computed with and <code>qnorm(p, mean = 0, sd = 1</code>.
      </p>
      <p>
        <strong>Example</strong>: Following Example 5.6.4 in DeGroot and Morris
        (DeGroot and Morris, <i>Probability and Statistics</i>, 308)
        the probability of a random variable with mean 5 and standard deviation
        2 being greater than 1 and less than 8 is
      </p>
      <p class='formula'>
         Pr(1 &lt; X &lt; 8) = Pr(X &lt; 8) - Pr(X &lt; 1)
      </p>
      <p>
        This can be computed with R command
      </p>
      <p class='formula'>
        <pre>
          &gt; pnorm(8, mean = 5, sd = 2) - pnorm(1, mean = 5, sd = 2)
          [1] 0.9104427
        </pre>
      </p>
      <p>
        So the probability that X is greater than 1 and less than 8 is 0.91. ▢
      </p>
      <p>
        The normal distribution is a good approximation for variables in many random processes.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 303)
        The <strong>Central Limit Theorem</strong> states that the distribution of a sum of 
        random variables Σ<sub>i=1</sub><sup>n</sup> X<sub>i</sub>  with any distribution will be
        approximately the normal distribition with mean nμ and variance nσ<sup>2</sup>, as
        n becomes large.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 361)
      </p>
      <p>
        The <strong>gamma distribution</strong> for the continuous random variable X with parameters 
        α and β is 
      </p>
      <p class='formula'>
         ƒ(x|α, β) = [β<sup>α</sup> / Γ(α)] x<sup>α-1</sup> e<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        where Γ(α) is the gamma function. 
        The mean of the gamma distribution is α/β and the variance is α/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 319-320)
      </p>
      <p>
        The <strong>exponential distribution</strong> for the continuous random variable X with parameter β is
      </p>
      <p class='formula'>
         ƒ(x|β) = βe<sup>-βx</sup> for x &gt; 0 or 0 otherwise
      </p>
      <p>
        The exponential distribution is a special case of the gamma distribution with α = 1.
        The mean of the exponential distribution is 1/β and the variance is 1/β<sup>2</sup>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 321)
        The exponential distribution can be drawn with the R command below.
      </p>
      <p class='formula'>
        <pre>
          &gt; curve(0.8*exp(-0.8*x), 0, 5, xlab="x", ylab="Probability Density", col="blue")
        </pre>
      </p>
      <p>
        The generated graph is shown in Figure 7.
      </p>
      <p class='picture'>
        <img src="images/exponential_distribution08.png"/><br/>
        Figure 7: Exponential Distribution (β = 0.8)
      </p>
      <p>
        The <strong>beta distribution</strong> for the continuous random variable X with parameters α and β is
      </p>
      <p class='formula'>
         ƒ(x|α, β) = [Γ<sup>β+α</sup> / (Γ(α) Γ(β))] x<sup>α-1</sup> (1 - x)<sup>β-1</sup> for 0 &lt; x &lt; 1 or 0 otherwise
      </p>
      <p>
        The mean of the beta distribution is 
      </p>
      <p class='formula'>
        E(X) = α/(α + β)
      </p>
      <p>
        The variance of the beta distribution is 
      </p>
      <p class='formula'>
        Var(X) = αβ/[(α + β)<sup>2</sup>(α + β + 1)]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 328-329)
      </p>
      <p>
        The <strong>multinomial distribution</strong> for the discrete random vector 
        <strong>X</strong> = (X<sub>1</sub>, X<sub>2</sub>, ... X<sub>k</sub>) having probabilities
        <strong>p</strong> = (p<sub>1</sub>, p<sub>2</sub>, ... p<sub>k</sub>) 
        with n items selected is defined as
      </p>
      <p class='formula'>
         f(<strong>X</strong>|n, <strong>p</strong>) = [n!/(x<sub>1</sub> x<sub>2</sub> ... x<sub>k</sub>)]
         p<sub>1</sub><sup>x<sub>1</sub></sup> p<sub>2</sub><sup>x<sub>2</sub></sup> ... p<sub>k</sub><sup>x<sub>k</sub></sup>
      </p>
      <p>
        if x<sub>1</sub> + x<sub>2</sub> + ... x<sub>k</sub> = n or 0 otherwise.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 334)
        The multinomial distribution is appropriate for distributions into sets that are not necessarily numbers,
        for example, the frequencies of words of different part of speech values.
      </p>
       <h3><a name="estimation"></a>Statistical Estimation</h3>
      <p>
        A <strong>statistical model</strong> is a collection of random variables, identification of probability
        distributions for the variables, and the set of parameters that the distributions require values for.
        <strong>Statistical inference</strong> is a probabilistic statement about a statistical model.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 377-378)
        For example, the approximate date of a document may be inferred from the vocabulary in it.
        (Krippendorff, <i>Content Analysis</i>, 42)
        In a Buddhist text mention of copying sutras or description of devotional practices may help provide information for the date
        for the text. The data may be recorded as 0 (not present) and 1 (present) or as a word frequency.
      </p>
      <p>
        A <strong>statistic</strong> is a function of a set of random variables. For example, mean, median, 
        and variance are statistics.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 382)
      </p>
      <p>
        Parameters in probability distributions are usually unknown and needed to be estimated with statistical methods.
        So the parameters themselves can be considered simply as unknown constants or to have probability distributions
        themselves.
        The <strong>prior distribution</strong> of a parameter θ is the probability distribution ζ(θ) assumed before
        experimental observations are applied to estimate its value.
        The <strong>posterior distribution</strong> ζ(θ|x<sub>1</sub>, ... x<sub>n</sub>) 
        is the conditional distribution after the random variables X<sub>1</sub>, ... X<sub>n</sub> have been observed.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 385-387)
        The <strong>likelihood function</strong> f<sub>n</sub>(<strong>x</strong>|θ) is the joint probability function
        of the random variables <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>) and the parameter θ.
        The likelihood function can be used to relate teh prior and posterior distributions,
      </p>
      <p class='formula'>
         ζ(θ|<strong>x</strong>) ∝ ƒ<sub>n</sub>(<strong>x</strong>|θ) ζ(θ)
      </p>
      <p>
        The constant of proportionality can be found from equating the total probability to 1.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 390)
      </p>
      <p>
        A <strong>conjugate family of prior distributions</strong> is a family of possible distributions for ζ(θ) where 
        the posterior distribution also belongs to the same family. 
        The family of gamma distributions is a conjugate family of prior distributions for Poisson distributions of
        the random variables X<sub>1</sub>, ... X<sub>n</sub> when the parameter θ is unknown.
        The family of normal distributions is itself a conjugate family of prior distributions for normal distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the mean is unknown but the variance is known.
        The family of gamma distributions is also a conjugate family of prior distributions for exponential distributions of
        X<sub>1</sub>, ... X<sub>n</sub> when the value of the parameter θ is unknown.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 395-402)
      </p>
      <p>
        An <strong>estimator</strong> δ(X<sub>1</sub>, ... X<sub>n</sub>) gives an estimate of the parameter θ using
        observed values of the data <strong>x</strong> = (x<sub>1</sub>, ... x<sub>n</sub>). 
        A <strong>loss function</strong> L(θ, a) quantifies the effect of the difference between the estimate a of θ
        and the true value.
        A <strong>Bayes estimator</strong> δ*(<strong>x</strong>) minimizes the expected value of the loss function.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 408-409)
      </p>
      <p>
        The squared error loss function is defined as 
      </p>
      <p class='formula'>
         L(θ, a) = (θ - a)<sup>2</sup>
      </p>
      <p>
        When the squared error loss function is used the Bayes estimator is the posterior mean value of θ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 411)
      </p>
      <p>
        Another approach to estimating parameters maximizes the probability of observed data.
        A <strong>likelihood function</strong> f<sub>n</sub>(<strong>x</strong>|θ) is a joint pdf for 
        continuous random variables or joint pf
        for discrete random variables with the parameter θ considered part of the joint distribution.
        A <strong>maximum likelihood estimator</strong> maximizes the value of f<sub>n</sub>(<strong>x</strong>|θ).
        (DeGroot and Morris, <i>Probability and Statistics</i>, 418)
        This approach avoids the need to assume a probability distribution for θ. However, the drawback is that
        is may not always give a good estimate for θ and sometimes it may not exist at all.
      </p>
      <h3><a name="sampling"></a>Sampling</h3>
      <p>
        A <strong>samplling distribution</strong> is the distribition of a statistic T of a set of random variables 
        <strong>X</strong> = (X<sub>1</sub>, ... X<sub>n</sub>) that are a sample of the random variable X
        with distribution having a parameter θ. 
        A sampling distribution can be used to determine how good an estimate θ̂ of the parameter θ is.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 465) 
      </p>
      <p>
        The <strong>chi-square</strong> χ<sup>2</sup> distribition with m degrees of freedom 
        is the gamma distribution with α = m/2 and β = ½.
        This can be written as
      </p>
      <p class='formula'>
         ƒ(x) = 1/(2<sup>m/2</sup> Γ(m/2)) x<sup>(m/2)-1</sup> e<sup>-x/2</sup>   for x &gt; 0.
      </p>
      <p>
        The mean of the χ<sup>2</sup> distribition is E(X) = m. 
        The variance is Var(X) = 2m.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 469-470) 
        The χ<sup>2</sup> distribition can computed with the R <code>dchisq()</code> function.
        The χ<sup>2</sup> distribition with 5 degrees of freedom can be drawn in R
        using the command below.
      </p>
      <p class='formula'>
        <pre>
          &gt; curve(dchisq(x, df=5), 0, 20, xlab="x", ylab="Chi Square", col="blue")
        </pre>
      </p>
      <p>
        The result is shown in Figure 8.
      </p>
      <p class='picture'>
        <img src="images/chi_square_df5.png"/><br/>
        Figure 8: χ<sup>2</sup> Distribition with 5 Degrees of Freedom
      </p>
      <p>
        The χ<sup>2</sup> distribition with two degrees of freedom is the same as the exponential distribution 
        with parameter ½.
      </p>
      <p>
        The cummulative χ<sup>2</sup> distribition can be computed with the <code>pchisq()</code> R function.
      </p>
      <p>
        <strong>Example</strong>: Following the Example 8.2.3 in DeGroot and Morris
        (DeGroot and Morris, <i>Probability and Statistics</i>, 470) 
        The probability that X is less than 10.0 for a 
        χ<sup>2</sup> distribition with 10 degrees of freedom can be found using the R command
      </p>
      <p class='formula'>
        <pre>
          &gt; pchisq(c(10), df=10)
          [1] 0.5595067
        </pre>
      </p>
      <p>
        Giving the probability 0.55 or 56%. ▢
      </p>
      <p>
        If X<sub>1</sub>, ... X<sub>n</sub> are a random sample from a normal distribution with mean μ and 
        variance σ<sup>2</sup> then the sample mean and variance have maximum likelihood estimators
      </p>
      <p class='formula'>
         μ&#770; = X&#772;<sub>n</sub> <br>
         σ&#770; = [(1/n) Σ<sub>i=1</sub><sup>n</sup>(X<sub>i</sub> - X&#772;<sub>n</sub>)<sup>2</sup>]<sup>½</sup>
      </p>
      <p>
        These estimators are independent variables.
        The estimator for the sample mean X&#772;<sub>n</sub> has a normal distribution with mean μ and variance σ/n.
        The quantity nσ&#770;<sup>2</sup>/σ<sup>2</sup> has a χ<sup>2</sup> distribition with n - 1 degrees of freedom.
      </p>
      <p>
        The <strong>t distribution</strong> is useful when we need to use an estimate of the variance because we
        do not know the true variance. The t distribution is
      </p>
      <p class='formula'>
         Γ((m+1)/2)/[(mπ)<sup>½</sup>Γ(m/2)] (1 + x<sup>2</sup>/m)<sup>-(m+1)/2</sup>
      </p>
      <p>
        If X<sub>1</sub>, ... X<sub>n</sub> is a sample for random variabe X with with mean μ and variance σ<sup>2</sup>.
        Define the statistic
      </p>
      <p class='formula'>
         σ′ = [Σ<sub>i=1</sub><sup>n</sup>(X<sub>i</sub> - X&#772;<sub>n</sub>)<sup>2</sup>]<sup>½</sup>
      </p>
      <p>
        Also define the random variable
      </p>
      <p class='formula'>
         U = n<sup>½</sup>(X&#772; - μ)/σ
      </p>
      <p>
        U has a t distribution with n - 1 degrees of freedom.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 480-482) 
        The t distribution looks like the normal distribution but the tails do not converge to zero as quickly, 
        especially for small values of n.
        Values of the pdf for t distribution can be computed with the R <code>dt</code> function.
        The command below draws a graph of the t distribution with 5 degrees of freedom.
      </p>
      <p class='formula'>
        <pre>
          &gt; curve(dt(x, df=5), -3, 3, xlab="z", ylab="Probability Density", col="blue")
        </pre>
      </p>
      <p>
        The chart produced is shown in Figure 9.
      </p>
      <p class='picture'>
        <img src="images/t_distribution_df5.png"/><br/>
        Figure 9: t Distribution with 5 Degrees of Freedom
      </p>
      <p>
        A <strong>confidence interval</strong> (A, B) for coefficient γ is an interval with 
      </p>
      <p class='formula'>
         Pr(A &lt; g(θ) &lt; B) ≥ γ
      </p>
      <p>
        for a random sample <strong>X</strong> = (X<sub>1</sub>, ... X<sub>n</sub>) from a distribution with 
        parameter θ.
      </p>
      <p>
        The confidence interval for the sample mean from a normal distribution with mean μ and variance σ<sup>2</sup> is
      </p>
      <p class='formula'>
         A = X&#772;<sub>n</sub> - T<sub>n-1</sub><sup>-1</sup>((1 + γ)/2) σ′/n<sup>½</sup> <br/>
         B = X&#772;<sub>n</sub> + T<sub>n-1</sub><sup>-1</sup>((1 + γ)/2) σ′/n<sup>½</sup>
      </p>
      <p>
        where T<sub>n</sub>(c) is the cdf of the t distribution with n degrees of freedom 
        and T<sub>n</sub><sup>-1</sup> is the quantile function.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 486)
        The quantile function T<sub>n</sub><sup>-1</sup>(x) for a value x with n degrees fo freedom can be computed 
        with the R function <code>qt(x, df = n)</code>.
      </p>
      <p>
        <strong>Example</strong>: Computing Example 8.5.3 from DeGroot and Morris 
        (DeGroot and Morris, <i>Probability and Statistics</i>, 487) with a sample of 26 rain 
        measurements with sample average 5.134 and estimated variance 1.60.
        The 95% confidence intervale can be computed using the R commands
      </p>
      <p class='formula'>
        <pre>
          &gt; A &lt;- 5.134 - qt(1.95/2, df=25) * 1.6/sqrt(25)
          &gt; A
          [1] 4.474948
          &gt; B &lt;- 5.134 + qt(1.95/2, df=25) * 1.6/sqrt(25)
          &gt; B
          [1] 5.793052
        </pre>
      </p>
      <p>
        The 95% confidence interval is (4.47, 5.79). ▢
      </p>
      <p>
        A <strong>one-sided confidence interval</strong> (A, ∞) has a statistic A, such that
      </p>
      <p class='formula'>
         Pr(A &lt; g(θ)) ≥ γ
      </p>
      <p>
        This is a 100γ percent one-sided confidence interval for g(θ).
        Similarly, the interval (-∞, B) is a 100γ percent one-sided confidence interval for g(θ) where
      </p>
      <p class='formula'>
         Pr(g(θ) &lt; B) ≥ γ
      </p>
      <p>
        For a normal distribution with mean μ and variance σ<sup>2</sup> the confidence limits 
        A and B can be computed as 
      </p>
      <p class='formula'>
         A = X&#772;<sub>n</sub> - T<sub>n-1</sub><sup>-1</sup>(γ) σ′/n<sup>½</sup> <br/>
         B = X&#772;<sub>n</sub> + T<sub>n-1</sub><sup>-1</sup>(γ) σ′/n<sup>½</sup>
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 488-489)
      </p>
      <p>
        An <strong>unbiased estimator</strong> δ(<strong>X</strong>) of a function g(θ) of a parameter θ has 
        the same expection as g(θ) or all values of θ.
        That is,  E<sub>θ</sub>[δ(<strong>X</strong>)] = g(θ) for all θ.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 507)
        For example, the sample mean X̄<sub>n</sub> is an unbiased estimate of the true mean μ because the mean of 
        X̄<sub>n</sub> is μ for all values of μ.
        The statistic
      </p>
      <p class='formula'>
         σ̂<sub>1</sub><sup>2</sup> = (1/(n - 1)) ∑<sub>i=1</sub><sup>n</sup>(X<sub>i</sub> - X&#772;<sub>n</sub>)<sup>2</sup>
      </p>
      <p>
        is an unbiased estimator of the variance. 
        (DeGroot and Morris, <i>Probability and Statistics</i>, 508)
        σ̂<sub>1</sub><sup>2</sup> is sometimes called the <strong>sample variance</strong>.
      </p>
      <p>
        <strong>Example</strong>: 
        The sample variance and sample standard deviation of word frequency in the NTI Reader tagged corpus 
        can be found with the R commands below.
      </p>
      <p class='formula'>
        <pre>
          &gt; v &lt;- var(unigram$frequency)
          &gt; v
          [1] 140.1174
          &gt; sqrt(v)
          [1] 11.83712
        </pre>
      </p>
      <p>
        Compare this to the population variance 140.20 and standard deviation 11.83 computed above. ▢
      </p>
      <h3><a name="hypothesis"></a>Hypothesis Testing</h3>
      <h4><a name="hypothesisconcepts"></a>Concepts</h4>
      <p>
        The <strong>null hypothesis</strong> H<sub>0</sub> is the hypothesis that θ ∈ Ω<sub>0</sub>, where
        θ is the parameter of a probability distribition and Ω<sub>0</sub> is a subset of the parameter space Ω.
        The <strong>alternative hypothesis</strong> H<sub>1</sub> is the hypothesis that θ ∈ Ω<sub>1</sub>,
        where Ω<sub>1</sub> is the complement of Ω<sub>0</sub>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 531-532)
      </p>
      <p>
        A <strong>critical region</strong> S<sub>1</sub> is a subset of the sample space S of a random vector
        <strong>X</strong> = (X<sub>1</sub>, ... X<sub>n</sub>) for which the null hypothesis H<sub>0</sub>
        is rejected.
        A <strong>test statistic</strong> T = r(<strong>X</strong>) defines a procedure where the 
        null hypothesis H<sub>0</sub> is rejected if T ∈ R, where R is a subset of the real numbers.
        R is the <strong>rejection region</strong> of the test.
        The critical region has the form S<sub>1</sub> = {<strong>x</strong>: r(<strong>x</strong>) ∈ R}.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 532-533)
      </p>
      <p>
        A <strong>power function</strong> π(θ, δ) is the probability that a test procedure δ will reject
        the null hypothesis for all values of the parameter θ. In symbols
      </p>
      <p class='formula'>
         π(θ, δ) = Pr(<strong>X</strong> ∈ S<sub>1</sub>)
      </p>
      <p>
        where S<sub>1</sub> is the critical region.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 534)
      </p>
      <p>
        A <strong>type I error</strong> is an erroneous choice to reject the null hypothesis.
        A <strong>type II error</strong> is an erroneous choice not to reject a false null hypothesis.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 535)
      </p>
      <p>
        The <strong>size</strong> α(δ) is defined as 
      </p>
      <p class='formula'>
         α(δ) = sup <sub>θ ∈ Ω<sub>0</sub></sub> π(θ, δ)
      </p>
      <p>
        where δ is a test and sup is the supremum.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 535)
      </p>
      <p>
        The <strong>p-value</strong> is the smallest level α<sub>0</sub> that would result in rejection of the 
        null hypothesis.
      </p>
      <p>
        The <strong>likelihood ratio statistic</strong> Λ(<strong>x</strong>) is the largest value of the 
        likelihood function in Ω<sub>0</sub> compared to the entire parameter space Ω. In symbols
      </p>
      <p class='formula'>
         Λ(<strong>x</strong>) = [sup <sub>θ ∈ Ω<sub>0</sub></sub> ƒ(<strong>x</strong> | θ)] /
                                [sup <sub>θ ∈ Ω</sub> ƒ(<strong>x</strong> | θ)]
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 544)
      </p>
      <p>
        A <strong>simple hypothesis</strong> is a choice from two alternative parameter values in the
        parameter space Ω = {θ<sub>0</sub>, θ<sub>1</sub>}. 
        It has the form
      </p>
      <p class='formula'>
         H<sub>0</sub>: θ = θ<sub>0</sub><br/>
         H<sub>1</sub>: θ = θ<sub>1</sub>
      </p>
      <p>
        The probability α of a type I error for a test procedure δ is 
        α(δ) = Pr(Reject H<sub>0</sub> | θ = θ<sub>0</sub>).
        The probability β of a type II error for a test procedure δ is 
        β(δ) = Pr(Do not reject H<sub>0</sub> | θ = θ<sub>1</sub>).
        (DeGroot and Morris, <i>Probability and Statistics</i>, 551)
      </p>
      <p>
        A <strong>uniformly most powerful test</strong> is for a test procedure δ<sup>*</sup> for the hypothesis
      </p>
      <p class='formula'>
         H<sub>0</sub>: θ ∈ Ω<sub>0</sub><br/>
         H<sub>1</sub>: θ ∈ Ω<sub>1</sub>
      </p>
      <p>
        at the level of significance α<sub>0</sub> if α(δ<sup>*</sup>) ≤ α<sub>0</sub> where
      </p>
      <p class='formula'>
         π(θ, δ) ≤ π(θ, δ<sup>*</sup>) for all θ ∈ Ω<sub>1</sub>
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 560)
      </p>
      <p>
        A <strong>one sided</strong> alternative hypothesis has the form
      </p>
      <p class='formula'>
         H<sub>0</sub>: θ ≤ θ<sub>0</sub><br/>
         H<sub>1</sub>: θ &gt; θ<sub>1</sub>
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 562)
      </p>
      <p>
        A <strong>two sided</strong> alternative hypothesis has the form
      </p>
      <p class='formula'>
         H<sub>0</sub>: θ = θ<sub>0</sub><br/>
         H<sub>1</sub>: θ ≠ θ<sub>1</sub>
      </p>
      <p>
        (DeGroot and Morris, <i>Probability and Statistics</i>, 565)
      </p>
      <p>
        An <strong>unbiased test</strong> is a test δ where
      </p>
      <p class='formula'>
         π(θ, δ) ≤ π(θ′, δ)
      </p>
      <p>
        for every θ ∈ Ω<sub>0</sub> and θ′ ∈ Ω<sub>1</sub>.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 573)
      </p>
      <p>
        A <strong>t test</strong> for the one sided hypothesis about the mean μ
      </p>
      <p class='formula'>
         H<sub>0</sub>: μ ≤ μ<sub>0</sub><br/>
         H<sub>1</sub>: μ &gt; μ<sub>0</sub>
      </p>
      <p>
        where the variance σ<sup>2</sup> is unknown, rejects H<sub>0</sub> if
      </p>
      <p class='formula'>
         U = n<sup>½</sup> (X&#772; - μ<sub>0</sub>)/σ′ ≥ c
      </p>
      <p>
        and c is found from the t quantile function for level of significance α<sub>0</sub>.
        The opposite hypothesis H<sub>0</sub>: μ ≥ μ<sub>0</sub> is rejected if U ≤ c.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 576)
      </p>
      <p>
        <strong>Example</strong>: We will work through Example 9.5.3 in DeGroot and Morris
        (DeGroot and Morris, <i>Probability and Statistics</i>, 578) with R commands.
        The problem tests the hypothesis H<sub>0</sub>: μ ≥ 200 with a level of significance 
        α<sub>0</sub> = 0.1 for a random sample with n = 18, X&#772; = 182.17,
        and σ′ = 72.22. 
        The 0.1 quantile of the t distribution and the test statistic U can be found with the R commands
      </p>
      <p class='formula'>
        <pre>
          &gt; c = qt(0.1, df=17)
          &gt; c
          [1] -1.333379
          &gt; U = sqrt(18-1) * (182.17 - 200)/72.2
          &gt;  U
          [1] -1.018213
        </pre>
      </p>
      <p>
        Since U ≤ c is false, the null hypothesis μ ≥ 200 is not rejected. ▢
      </p>
      <p>
        A t test for the two sided hypothesis about the mean μ
      </p>
      <p class='formula'>
         H<sub>0</sub>: μ = μ<sub>0</sub><br/>
         H<sub>1</sub>: μ ≠ μ<sub>0</sub>
      </p>
      <p>
        where the variance σ<sup>2</sup> is unknown, rejects H<sub>0</sub> if 
      </p>
      <p class='formula'>
         |U| ≥ T<sub>n-1</sub><sup>-1</sup>(1 - α<sub>0</sub>/2)
      </p>
      <p>
        where T<sub>n-1></sub><sup>-1</sup> is the quantile function of the t distribution.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 582)
      </p>
      <h4><a name="twomeans"></a>Comparing means from two distributions</h4>
      <p>
        To test a hypothesis comparing the means μ<sub>1</sub> and μ<sub>0</sub> of two populations
        <strong>X</strong> = X<sub>1</sub>, ... X<sub>m</sub> and 
        <strong>Y</strong> = Y<sub>1</sub>, ... Y<sub>n</sub>
        with the same variance σ, the null and alternate hypotheses
      </p>
      <p class='formula'>
         H<sub>0</sub>: μ<sub>1</sub> ≤ μ<sub>2</sub><br/>
         H<sub>1</sub>: μ<sub>1</sub> &gt; μ<sub>2</sub>
      </p>
      <p>
        are used. The <strong>two-sample statistic</strong> U is defined as 
      </p>
      <p class='formula'>
         U = (m + n - 2)<sup>½</sup>(X&#772;<sub>m</sub> - Y&#772;<sub>n</sub>) /
         [(1/m + 1/n)<sup>½</sup>(S<sub>X</sub><sup>2</sup> + S<sub>Y</sub><sup>2</sup>)<sup>½</sup>]
      </p>
      <p>
        U has a t distribution with m + n - 2 degrees of freedom. In this statistic
      </p>
      <p class='formula'>
         X&#772;<sub>m</sub> = (1/m) ∑<sub>i=1</sub><sup>m</sup>X<sub>i</sub> and
         Y&#772;<sub>n</sub> = (1/n) ∑<sub>i=1</sub><sup>n</sup>Y<sub>i</sub>
      </p>
      <p>
        and 
      </p>
      <p class='formula'>
         S<sub>X</sub><sup>2</sup> = ∑<sub>i=1</sub><sup>m</sup>(X<sub>i</sub> - X&#772;<sub>m</sub>)<sup>2</sup> and
         S<sub>Y</sub><sup>2</sup> = ∑<sub>i=1</sub><sup>n</sup>(Y<sub>i</sub> - Y&#772;<sub>n</sub>)<sup>2</sup>
      </p>
      <p>
        The null hypothesis H<sub>0</sub> is rejected at the level of significance α<sub>0</sub> if 
        U ≥ T<sub>m+n-2</sub><sup>-1</sup>(1-α<sub>0</sub>).
        (DeGroot and Morris, <i>Probability and Statistics</i>, 588-589)
      </p>
      <p>
        <strong>Example</strong>: We will work through Example 9.6.2 in DeGroot and Morris
        (DeGroot and Morris, <i>Probability and Statistics</i>, 589-590) with R commands.
        The problem tests the hypothesis that the mean of rain from seeded clouds μ<sub>1</sub> 
        is greater than the mean of rain from unseeded clouds μ<sub>2</sub>. That is, we attempt to reject
        H<sub>0</sub>: μ<sub>1</sub> ≤ μ<sub>2</sub> at level of significance α<sub>0</sub> = 0.01.
        The mean of the 26 measurements with seeded clouds is X&#772;<sub>m</sub> = 5.13.
        The mean of the 26 measurements with unseeded clouds is X&#772;<sub>n</sub> = 3.99.
        Also, S<sub>X</sub><sup>2</sup> = 63.96 and S<sub>Y</sub><sup>2</sup> = 67.39.
        The R commands to compute U and critical value of T<sub>m+n-2</sub><sup>-1</sup> are
      </p>
      <p class='formula'>
        <pre>
          &gt; U = sqrt(26+26-2)*(5.13-3.99)/(sqrt(1/26+1/26)*sqrt(63.96+67.39))
          &gt;  U
          [1] 2.535984
          &gt;  c = qt(0.99, df=26+26-2)
          &gt;  c
          [1] 2.403272
        </pre>
      </p>
      <p>
        Since U > c, the null hypothesis is rejected.▢
      </p>
      <h4><a name="fdist"></a>F Distribution</h4>
      <p>
        An <strong>F distribution</strong> for the variable X combining two random variables Y and W with
        χ<sup>2</sup> distributions with m and n degrees of freedom in the form
      </p>
      <p class='formula'>
         X = (Y/m)/(W/n)
      </p>
      <p>
        X has an F distribution, which is given by the formula
      </p>
      <p class='formula'>
         ƒ(x) = (Γ[(m + n)/2] m<sup>m/2</sup>n<sup>n/2</sup>) / [Γ(m/2)Γ(n/2)] · x<sup>(m/2)-1</sup>/(mx + n)<sup>(m+n)/2</sup>
      </p>
      <p>
        for x &gt; 0.
      </p>
      <p>
        An <strong>F test</strong> compares the unknown variances of two random variables X<sub>1</sub>, ... X<sub>n</sub>
        and Y<sub>1</sub>, ... Y<sub>n</sub>.
        The null and alternate hypotheses are
      </p>
      <p class='formula'>
         H<sub>0</sub>: σ<sub>1</sub><sup>2</sup> ≤ σ<sub>2</sub><sup>2</sup><br/>
         H<sub>1</sub>: σ<sub>1</sub><sup>2</sup> &gt; σ<sub>2</sub><sup>2</sup>
      </p>
      <p>
        In the test the statistic V is defined as 
      </p>
      <p class='formula'>
         V = [S<sub>X</sub><sup>2</sup>/(m - 1)] / [S<sub>Y</sub><sup>2</sup>/(n - 1)]
      </p>
      <p>
        If V ≥ c, where c is determined from the level of significance α<sub>0</sub>, then H<sub>0</sub> is rejected.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 598-599)
        The quantile function for the F distribution can be computed with the R command
        <code>qf(p, df1, df2</code>.
      </p>
      <p>
        <strong>Example</strong>: We will work through Example 9.7.3 in DeGroot and Morris
        (DeGroot and Morris, <i>Probability and Statistics</i>, 600-601) with R commands.
        X<sub>1</sub>, ... X<sub>6</sub> have unknown mean and variance. 
        S<sub>X</sub><sup>2</sup> = 30 is computed from observations.
        Y<sub>1</sub>, ... Y<sub>21</sub> also have unknown mean and variance and have
        S<sub>Y</sub><sup>2</sup> = 40.
        The following R commands compute the statistic V and the quantile value for 
        α<sub>0</sub> = 0.025 and α<sub>0</sub> = 0.05.
      </p>
      <p class='formula'>
        <pre>
          &gt; V = (30/(6-1))/(40/(21-1))
          &gt; V
          [1] 3
          &gt; qf(0.95, df1=5, df2=20)
          [1] 2.71089
          &gt; qf(0.975, df1=5, df2=20)
          [1] 3.289056
        </pre>
      </p>
      <p>
        Since V &gt; 2.71 the null hypothesis is rejected at the α<sub>0</sub> = 0.05 significance level but since
        V &lt; 3.29 it is not rejected at the α<sub>0</sub> = 0.025 level. ▢
      </p>
      <h3><a name="categorical"></a>Categorical Data</h3>
      <p>
        <strong>Categorical data</strong> is data where observations classifies the data into different categories.
        <strong>Nonparametric methods</strong> are methods that do not depend on data belonging to
        specific parametric families of distributions.
        Both of these areas are potentially very useful in natural language processing.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 625)
      </p>
      <h4><a name="goodness"></a>χ<sup>2</sup> goodness of fit test</h4>
      <p>
        The <strong>χ<sup>2</sup> goodness of fit test</strong> measures how well the proportions of categories in a random 
        sample matches against proportions from the whole population. In this test there are k different
        categories and p<sub>i</sub> is the proportion of each in the population. The proportions in the
        random sample belonging to each category are p<sub>1</sub><sup>0</sup>, ... p<sub>k</sub><sup>0</sup>.
        The hypothesis tested is
      </p>
      <p class='formula'>
         H<sub>0</sub>: p<sub>i</sub> = p<sub>i</sub><sup>0</sup> for i = 1, ... k<br/>
         H<sub>1</sub>: p<sub>i</sub> ≠ p<sub>i</sub><sup>0</sup> for at least one i
      </p>
      <p>
        That is, the null hypothesis is that the distribution of data amongst the different categories is explained
        by the probabilities given. The alternative hypothesis is that the data in at least one of the categories 
        cannot be explained by the given probabilities.
        The <strong>χ<sup>2</sup> statistic</strong> is defined as
      </p>
      <p class='formula'>
         Q = ∑<sub>i=1</sub><sup>k</sup>(N<sub>i</sub> - np<sub>i</sub><sup>0</sup>)<sup>2</sup> / np<sub>i</sub><sup>0</sup>
      </p>
      <p>
        where N<sub>i</sub> are the numbers of each item in the sample and n is the total number in the sample.
        If H<sub>0</sub> is true then Q converges to a χ<sup>2</sup> distribution as n → ∞.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 626)
        Smaller values for Q are less likely to result in rejection of the null hypothesis.
        The <code>chisq.test</code> R function can be used to perform this test.
      </p>
      <p>
        <strong>Example</strong>: This example uses the χ<sup>2</sup> goodness of fit test to compare the 
        distribution counts for words grouped by part of speech
        in the tagged corpus and a document that is included in the tagged corpus.
        The most frequent few pronouns in the tagged corpus can be extracted with the following R command.
      </p>
      <p class='formula'>
        <pre>
          &gt; head(unigram[grep("/PN\\[", unigram$pos.tagged.text),])
                 pos.tagged.text element.text word.id frequency
          2     是/PN[shì | this]           是   17908       136
          12      所/PN[suǒ | it]           所   17100        66
          26        我/PN[wǒ | I]           我     321        45
          28     其/PN[qí | that]           其    1574        43
          33 云何/PN[yúnhé | how]         云何   29319        39
          62      汝/PN[rǔ | you]           汝    6690        22
        </pre>
      </p>
      <p>
        The regular expression <code>"/PN\\["</code> helps the <code>grep()</code> identify patterns like "/PN[""
        encapsulating part-of-speech tags.
        The R script <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/r/pos_counts.R">pos_counts.R</a>
        aggregates the frequencies of each separate part of speech in the tagged corpus and for the Heart Sutra
        and compares the counts with a χ<sup>2</sup> test. It can be run with the following command.
      </p>
      <p class='formula'>
        <pre>
          $ Rscript r/pos_counts.R 

          Chi-squared test for given probabilities

          data:  heart.count
          X-squared = 71.0523, df = 7, p-value = 9.052e-13
        </pre>
      </p>
      <p>
        With a very small p-value,
        the output shows that the distribution of part-of-speech counts in the Heart Sutra is not well explained
        by the general distribution of part-of-speech counts in the tagged corpus.
        Small count values have been avoided by combining categories.
        The data are shown in Table 1.
      </p>
      <table>
        <caption>Table 1: Predicted and Observed Word Counts for the Heart Sutra</caption>
        <tbody>
          <tr><th>Part of Speech</th><th>Observed Count</th><th>Predicted Count</th></tr>
          <tr><td>Adjectives</td><td>15</td><td>10.3</td></tr>
          <tr><td>Existential verbs</td><td>21</td><td>5.5</td></tr>
          <tr><td>Regular verbs</td><td>32</td><td>36.9</td></tr>
          <tr><td>Proper nouns</td><td>7</td><td>19.8</td></tr>
          <tr><td>Regular nouns</td><td>63</td><td>51.2</td></tr>
          <tr><td>Pronouns and numbers</td><td>10</td><td>23.8</td></tr>
          <tr><td>Adverbs</td><td>18</td><td>11.6</td></tr>
          <tr><td>Other function words</td><td>15</td><td>21.8</td></tr>
        </tbody>
      </table>
      <p>
        There is a relatively higher proportion of existential verbs in the Heart Sutra, which is expected. ▢
      </p>
      <h4><a name="chisqcomposite"></a>χ<sup>2</sup> Test for a Composite Hypothesis</h4>
      <p>
        The χ<sup>2</sup> test for a composite hypothesis test can be used to do a goodness-of-fit test 
        against a distribution parameterized by a set of parameters 
        <strong>θ</strong> = (θ<sub>1</sub>, ... θ<sub>s</sub>). The probabilities for the categories are given by 
        the functions π<sub>1</sub>(<strong>θ</strong>), ... π<sub>k</sub>(<strong>θ</strong>).
        The hypothesis is 
      </p>
      <p class='formula'>
         H<sub>0</sub>: There is a <strong>θ</strong> ∈ Ω such that p<sub>i</sub> = θ<sub>i</sub>
         H<sub>1</sub>: H<sub>0</sub> is false
      </p>
      <p>
        The Q statistic is defined as 
      </p>
      <p class='formula'>
         Q = ∑<sub>i=1</sub><sup>k</sup>[N<sub>i</sub> - nπ<sub>i</sub>(<strong>θ̂</strong>)]<sup>2</sup> / 
         nπ<sub>i</sub>(<strong>θ̂</strong>)
      </p>
      <p>
        If H<sub>0</sub> is true then the cdf of Q will converge on the cdf of χ<sup>2</sup> with k - s - 1
        degrees of freedom, as n → ∞.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 634-635)
      </p>
      <h4><a name="contingency"></a>Contingency Tables</h4>
      <p>
        A <strong>contingency table</strong> is a table that classifies each observation in two or more ways.
        A <strong>two-way</strong> contingency table classifies data in two ways.
        A contingency table can be used to test data independence using a χ<sup>2</sup> test.
        If N<sub>ij</sub> is the count in each table cell then the row and column counts are
      </p>
      <p class='formula'>
         N<sub>i+</sub> = ∑<sub>i=1</sub><sup>C</sup>N<sub>ij</sub><br/>
         N<sub>j+</sub> = ∑<sub>i=1</sub><sup>R</sup>N<sub>ij</sub>
      </p>
      <p>
        where C is the number of columns, R is the number of rows, and n is the total number of cells,
        excluding the row and cell counts. An estimator for the expected number in each cell, based on the 
        marginal probabilities from the row and column counts is 
      </p>
      <p class='formula'>
         Ê<sub>ij</sub> = N<sub>i+</sub>N<sub>j+</sub>/n
      </p>
      <p>
        The statistic Q is defined as
      </p>
      <p class='formula'>
         Q = ∑<sub>i=1</sub><sup>C</sup>∑<sub>i=1</sub><sup>R</sup>(N<sub>ij</sub> - Ê<sub>ij</sub>)<sup>2</sup> / 
         Ê<sub>ij</sub>
      </p>
      <p>
        has a χ<sup>2</sup> distribution with (R - 1)(C - 1) degrees of freedom.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 642-643)
      </p>
      <p>
        <strong>Example</strong>: This example tests the independence of the distribution of word counts
        for different parts of speech for three texts, The Heart Sutra, the Amitabha Sutra, and the Diamond Sutra.
        The R script <a href="https://github.com/alexamies/buddhist-dictionary/blob/master/r/pos_independence.R">pos_independence.R</a> 
        computes the word counts and invokes the χ<sup>2</sup> test. It can be invoked as shown below.
      </p>
      <p class='formula'>
        <pre>
          $ $ Rscript r/pos_independence.R 
                            Heart Amitabha Diamond
          Adjectives           15       86     140
          Existential Verbs    21       31     130
          Verbs                32      245     709
          Proper Nouns          7      194     404
          Nouns                63      361     942
          Pronouns              10      226     503
          Adverbs              18       69     248
          Function Words       15      164     510

            Pearson's Chi-squared test

          data:  data
          X-squared = 111.0789, df = 14, p-value &lt; 2.2e-16
        </pre>
      </p>
      <p>
        The low p-value in results show that the null hypothesis should be rejected. The distribution of is not
        independent at the conventional 0.05 level of significance and even at more strict levels of significance. ▢
      </p>
      <p>
        The χ<sup>2</sup> test for data homogeneity is exactly the same as the test for data independence.
        (DeGroot and Morris, <i>Probability and Statistics</i>, 649)
      </p>
      <h3><a name="linear"></a>Linear Regression</h3>
      <p>
        <strong>Linear regression</strong> is a method finding a statistical best fit for a line against a set of data.
        (Knell, <i>Introductory R</i>, 224)
      </p>
      <p>
        <strong>Example</strong>: We saw above that there was a correlation between text size
        and the number of unique words in different texts in the corpus.
        However, it was not linear. It turns out that we can get a more linear relation in log space.
        The following R commands do a linear regression analysis on the log of the log of the 
        word count versus the log of the count of unique words and plot the line of best
        fit on the graph with the data points.
      </p>
      <p class='formula'>
        <pre>
          &gt; x &lt;- log(corpusstats$word.count)
          &gt; y &lt;- log(corpusstats$unique.words)
          &gt; unique.lm &lt;- lm(y ~ x)
          &gt; plot(x, y, xlab="Log Word Count", ylab="Log Unique Words", pch=17, col="blue")
          &gt; abline(unique.lm)
        </pre>
      </p>
      <p>
        The R function <code>lm(y ~ x)</code> performs a linear regression of <code>y</code>
        as a function of <code>x</code>.
        The function <code>abline()</code> takes the output of the linear regression and adds
        it to the plot.
        The graph generated is shown in Figure 10.
      </p>
      <p class='picture'>
        <img src="images/log_unique_words.png"/><br/>
        Figure 10: Linear Regression of Log Count of Unique Words versus Log of Text Length
      </p>
      <h3><a name="references"></a>References</h3>
      <ol>
        <li>
          Amies, Alex. NTI Buddhist Text Reader GitHub Project, 2014. 
          <a href="https://github.com/alexamies/buddhist-dictionary">https://github.com/alexamies/buddhist-dictionary</a>.
        </li>
        <li>
          Bird, Steven, Ewan Klein, and Edward Loper. <i>Natural Language Processing with Python</i>. O’Reilly Media, Inc., 2009.
        </li>
        <li>
          DeGroot, Morris H., and Mark J. Schervish. <i>Probability and Statistics</i>. 4 edition. Boston: Pearson, 2011.
        </li>
        <li>
          Knell, Robert. <i>Introductory R: A Beginner’s Guide to Data Visualisation, Statistical Analysis and Programming in R</i>. United Kingdom: Self published, 2013.
        </li>
        <li>
          Krippendorff, Klaus H. <i>Content Analysis: An Introduction to Its Methodology</i>. Third Edition edition. SAGE Publications, Inc, 2012.
        </li>
        <li>
          Yau, Chi. <i>R Tutorial with Bayesian Statistics Using OpenBUGS</i>. Self, 2014. 
          <a href="http://www.r-tutor.com/content/r-tutorial-ebook">http://www.r-tutor.com/content/r-tutorial-ebook</a>.
        </li>
        <li>
          The R Project for Statistical Computing (version Pumpkin Helmet). R Foundation, 2014. 
          <a href="http://www.r-project.org/">http://www.r-project.org</a>.
        </li>
      <hr/>
      <p>
        Copyright Nan Tien Institute 2013 - 2014, 
        <a href="http://www.nantien.edu.au/" title="Fo Guang Shan Nan Tien Institute">www.nantien.edu.au</a>.
      </p>
      <p>This page was last updated on November 15, 2014.</p>
     </div>
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script>
  </body>
</html>
